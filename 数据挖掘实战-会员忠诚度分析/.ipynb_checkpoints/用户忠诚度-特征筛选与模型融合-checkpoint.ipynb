{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/08/2mewUjMshRdSy3r.png\" alt=\"image-20211208121621466\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> 二、Filter特征筛选+随机森林建模+网格搜索调优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Filter特征筛选\n",
    "\n",
    "- 特征筛选思路与方法\n",
    "\n",
    "在特征总数较多且特征矩阵较为稀疏时，需要考虑在模型训练前进行特征筛选。我们知道，树模型及树模型的集成模型存在一定的特征筛选机制，即每棵树在进行训练的时候会优先选择能最大程度提升子集纯度的特征进行划分，但当特征太多时，尽管最终结果不一定会受到冗余（无用）特征影响，但模型效率会大幅降低，因此面对树模型及树模型的集成模型，我们仍然需要考虑在实际建模前进行特征筛选，优先带入有效特征进行建模。\n",
    "\n",
    "而一般来说，特征筛选的方式主要有两类，其一是通过某些统计量对特征进行评估，在实际模型训练开始之前挑选出那些更加有效的特征并最终完成筛选，例如我们可以通过相关系数计算，判断特征和标签之间的相关关系，然后选取相关系数较大的特征带入进行建模，这种方法也被称为Filter方法；此外，我们也可以通过模型来筛选有效特征，例如随机森林模型可以输出特征重要性，我们可以先快速训练一个随机森林模型，然后根据输出的特征重要性，筛选更重要的特征带入后续超参数优化及交叉验证过程（需要知道的是，对于单独一个模型来说，是否带入冗余特征对单次训练来说影响不大，但由于超参数优化和交叉验证需要重复进行多轮训练，此时冗余特征的影响就会指数级上升），这样的特征筛选过程也被称为Wrapper过程。\n",
    "\n",
    "在接下来的随机森林与LightGBM的模型训练过程中，我们将分别使用Filter方法和Wrapper方法进行特征筛选，但其实不同方法也是可以互换的，即我们也可以采用RF+Filter和LightGBM+Wrapper策略，同学们可以课后自行尝试这些组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= 'D:\\数据挖掘实战\\Kaggle实例-3\\案例一【Elo用户忠诚度预测】\\Part 1\\数据\\建模数据\\\\'\n",
    "train = pd.read_csv(path+\"train.csv\")\n",
    "test = pd.read_csv(path+\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算稀疏性\n",
    "1 - np.count_nonzero(train) / train.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们需要在实际建模之前进行特征筛选，以及排除过于稀疏的特征。考虑到标签是连续变量，此处可以直接使用皮尔逊相关系数来进行特征筛选，筛选过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取特征名称\n",
    "features = train.columns.tolist()\n",
    "features.remove(\"card_id\")\n",
    "features.remove(\"target\")\n",
    "featureSelect = features[:]\n",
    "\n",
    "# 计算相关系数\n",
    "corr = []\n",
    "for fea in featureSelect:\n",
    "    corr.append(abs(train[[fea, 'target']].fillna(0).corr().values[0][1]))\n",
    "\n",
    "# 取top300的特征进行建模，具体数量可选\n",
    "se = pd.Series(corr, index=featureSelect).sort_values(ascending=False)\n",
    "feature_select = ['card_id'] + se[:300].index.tolist()\n",
    "\n",
    "# 输出结果\n",
    "train_RF = train[feature_select + ['target']]\n",
    "test_RF = test[feature_select]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，我们也可以将上述过程封装为一个函数并写入Elo.py模块（自定义模块），方便后续反复调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_pearson(train, test):\n",
    "    \"\"\"\n",
    "    利用pearson系数进行相关性特征选择\n",
    "    :param train:训练集\n",
    "    :param test:测试集\n",
    "    :return:经过特征选择后的训练集与测试集\n",
    "    \"\"\"\n",
    "    print('feature_select...')\n",
    "    features = train.columns.tolist()\n",
    "    features.remove(\"card_id\")\n",
    "    features.remove(\"target\")\n",
    "    featureSelect = features[:]\n",
    "\n",
    "    # 去掉缺失值比例超过0.99的\n",
    "    for fea in features:\n",
    "        if train[fea].isnull().sum() / train.shape[0] >= 0.99:\n",
    "            featureSelect.remove(fea)\n",
    "\n",
    "    # 进行pearson相关性计算\n",
    "    corr = []\n",
    "    for fea in featureSelect:\n",
    "        corr.append(abs(train[[fea, 'target']].fillna(0).corr().values[0][1]))\n",
    "\n",
    "    # 取top300的特征进行建模，具体数量可选\n",
    "    se = pd.Series(corr, index=featureSelect).sort_values(ascending=False)\n",
    "    feature_select = ['card_id'] + se[:300].index.tolist()\n",
    "    print('done')\n",
    "    return train[feature_select + ['target']], test[feature_select]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.随机森林模型训练与超参数调优\n",
    "\n",
    "- 网格搜索方法介绍\n",
    "\n",
    "接下来在挑选的特征中进行模型训练，当然为了确保模型本身泛化能力，一般模型训练过程都是和超参数调优过程同步进行的，如果我们是借助sklearn框架执行上述过程，则在流程的各环节都能得到极大的简化，一种最终基本的方案就是利用网格搜索进行超参数调优。当然在sklearn中网格搜索总共有三种，分别是网格搜索（GridSearchCV）、随机网格搜索（RandomGirdSearchCV）、对半网格搜索（HalvingGridSearchCV）以及随机对半网格搜索（RandomHalvingGridSearchCV），四种网格搜索尽管流程上有差异，但基本思路一致，都是通过不断的计算各组不同超参数组合输出的最终结果，并配合交叉验证过程，来寻找一组泛化能力最强的超参数组合。\n",
    "\n",
    "当然，除了网格搜索以外，常用的超参数搜索方法还有TPE搜索、贝叶斯优化器搜索等，不同于网格搜索的暴力枚举过程，这些优化器能够借助贝叶斯过程进行一定程度的先验计算，并在实际搜索过程中不断的调整先验的判断，最终通过先验的判断提前剔除（或者选择）一部分组合，进而加快整体搜索过程。本次案例中我们将率先使用网格搜索，在后续的模型中将继续展示其他优化器的超参数搜索调优过程。不过需要知道的是，在sklearn框架内，模型和优化器接口较为统一，因此整体流程会更加简洁，但从计算效率角度考虑，贝叶斯优化器的搜索效率往往更高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/08/BAUqI5cf6uFavND.png\" alt=\"image-20211208154708525\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_search(train):\n",
    "    \"\"\"\n",
    "    网格搜索参数调优\n",
    "    :param train:训练集\n",
    "    :return:网格搜索训练结果\n",
    "    \"\"\"\n",
    "    # Step 1.创建网格搜索空间\n",
    "    print('param_grid_search')\n",
    "    features = train.columns.tolist()\n",
    "    features.remove(\"card_id\")\n",
    "    features.remove(\"target\")\n",
    "    parameter_space = {\n",
    "        \"n_estimators\": [81], \n",
    "        \"min_samples_leaf\": [31],\n",
    "        \"min_samples_split\": [2],\n",
    "        \"max_depth\": [10],\n",
    "        \"max_features\": [80]\n",
    "    }\n",
    "    \n",
    "    # Step 2.执行网格搜索过程\n",
    "    print(\"Tuning hyper-parameters for mse\")\n",
    "    # 实例化随机森林模型\n",
    "    clf = RandomForestRegressor(\n",
    "        criterion=\"mse\",#选择均方误差模型\n",
    "        n_jobs=15, # 使用处理器个数限制 ，-1为无限制\n",
    "        random_state=22) # 随机种子，结果复现\n",
    "    # 带入网格搜索\n",
    "    grid = GridSearchCV(clf, parameter_space, cv=2, scoring=\"neg_mean_squared_error\")\n",
    "    grid.fit(train[features].values, train['target'].values)\n",
    "    \n",
    "    # Step 3.输出网格搜索结果\n",
    "    print(\"best_params_:\")\n",
    "    print(grid.best_params_)\n",
    "    means = grid.cv_results_[\"mean_test_score\"]\n",
    "    stds = grid.cv_results_[\"std_test_score\"]\n",
    "    # 此处额外考虑观察交叉验证过程中不同超参数的\n",
    "    for mean, std, params in zip(means, stds, grid.cv_results_[\"params\"]):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = param_grid_search(train_RF)\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果输出\n",
    "test['target'] = grid.best_estimator_.predict(test[features])\n",
    "test[['card_id', 'target']].to_csv(path+\"submission_randomforest.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 随机寻优\n",
    "\n",
    "较适用于小量样本，可以选取具有大样本代表性的一部分小样本进行随机寻优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "iris = load_iris()\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)# Fit the random search model\n",
    "rf_random.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the best score throughout the grid search\n",
    "print rf_random.best_score_\n",
    "#print the best parameter used for the highest score of the model.\n",
    "print rf_random.best_param_\n",
    " \n",
    "Output:\n",
    "{'bootstrap': True,\n",
    " 'max_depth': 70,\n",
    " 'max_features': 'auto',\n",
    " 'min_samples_leaf': 4,\n",
    " 'min_samples_split': 10,\n",
    " 'n_estimators': 400}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.随机森林交叉验证评估与中间结果保存\n",
    "\n",
    "在实际模型优化的过程中，有很多方法可以考虑，包括使用更加复杂高效的模型、进行模型融合、特征优化等等，但除此以外，还有一类经常被忽视但又同样高效优化的方法，那就是借助交叉验证进行多模型结果集成，当然此处所谓的多模型并不是采用了不同的评估器，而是同一个评估器（例如随机森林）在不同数据集上进行多次训练后生成多个模型，然后借助多个模型对测试集数据输出预测结果，最终通过取均值的方式来计算最终模型对测试集的预测结果。例如当前我们已经挑选了一组最优超参数，那么接下来就可以在这组超参数基础上进行五折交叉验证模型训练，该过程中对验证集的预测结果可以参与到后续Stacking融合过程中，而对测试集的预测结果则可以作为最终预测结果进行提交，相关过程如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/08/ALF3cfuSwmB7b8z.png\" alt=\"image-20211208192640281\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from numpy.random import RandomState\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(train, test, best_clf):\n",
    "    \"\"\"\n",
    "    进行训练和预测输出结果\n",
    "    :param train:训练集\n",
    "    :param test:测试集\n",
    "    :param best_clf:最优的分类器模型\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1.选择特征\n",
    "    print('train_predict...')\n",
    "    features = train.columns.tolist()\n",
    "    features.remove(\"card_id\")\n",
    "    features.remove(\"target\")\n",
    "\n",
    "    # Step 2.创建存储器\n",
    "    # 测试集评分存储器\n",
    "    prediction_test = 0\n",
    "    # 交叉验证评分存储器\n",
    "    cv_score = []\n",
    "    # 验证集的预测结果\n",
    "    prediction_train = pd.Series()\n",
    "    \n",
    "    # Step 3.交叉验证\n",
    "    # 实例化交叉验证评估器\n",
    "    kf = KFold(n_splits=5, random_state=22, shuffle=True)\n",
    "    # 执行交叉验证过程，kf.split返回两个值，分别是选取样本部分的索引，以及剩余样本部分的索引,且剩余样本部分的索引不会重复，全集恰好是数据的全集索引\n",
    "    for train_part_index, eval_index in kf.split(train[features], train['target']):\n",
    "        # 在训练集上训练模型\n",
    "        best_clf.fit(train[features].loc[train_part_index].values, train['target'].loc[train_part_index].values)\n",
    "        # 模型训练完成后，输出测试集上预测结果并累加至prediction_test中\n",
    "        prediction_test += best_clf.predict(test[features].values)\n",
    "        # 输出验证集上预测结果，eval_pre为临时变量\n",
    "        eval_pre = best_clf.predict(train[features].loc[eval_index].values)\n",
    "        # 输出验证集上预测结果评分，评估指标为MSE\n",
    "        score = np.sqrt(mean_squared_error(train['target'].loc[eval_index].values, eval_pre))\n",
    "        # 将本轮验证集上的MSE计算结果添加至cv_score列表中\n",
    "        cv_score.append(score)\n",
    "        print(score)\n",
    "        # 将验证集上的预测结果放到prediction_train中\n",
    "        prediction_train = prediction_train.append(pd.Series(best_clf.predict(train[features].loc[eval_index]),\n",
    "                                                             index=eval_index))\n",
    "    \n",
    "    # 打印每轮验证集得分、5轮验证集的平均得分\n",
    "    print(cv_score, sum(cv_score) / 5)\n",
    "    # 验证集上预测结果写入本地文件\n",
    "    pd.Series(prediction_train.sort_index().values).to_csv(path +\"train_randomforest.csv\", index=False)\n",
    "    # 测试集上平均得分写入本地文件\n",
    "    pd.Series(prediction_test / 5).to_csv(path +\"test_randomforest.csv\", index=False)\n",
    "    # 在测试集上加入target，也就是预测标签\n",
    "    test['target'] = prediction_test / 5\n",
    "    # 将测试集id和标签组成新的DataFrame并写入本地文件，该文件就是后续提交结果\n",
    "    test[['card_id', 'target']].to_csv(path +\"submission_randomforest.csv\", index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict(train_RF, test_RF, grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>三、集成学习与模型融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from hyperopt import hp, fmin, tpe\n",
    "from numpy.random import RandomState\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= 'D:\\数据挖掘实战\\Kaggle实例-3\\案例一【Elo用户忠诚度预测】\\Part 1\\数据\\建模数据\\\\'\n",
    "train = pd.read_csv(path+\"train.csv\")\n",
    "test = pd.read_csv(path+\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> 一、单模训练策略（二）\n",
    "## <center>**Wrapper特征筛选+LightGBM建模+TPE调优**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们进一步尝试Wrapper特征筛选+LightGBM建模+TPE调优的建模策略。相比随机森林这种老牌集成模型，LightGBM则算是Boosting家族中的新秀，LightGBM源自微软旗下的一个项目（Distributed Machine Learning Toolkit （DMKT）），同样是GBDT的一种实现方式，LightGBM支持高效率的并行训练，并且具有更快的训练速度、更低的内存消耗、更好的准确率、支持分布式可以快速处理海量数据等优点。\n",
    "\n",
    "此外，本次建模过程中将使用hyperopt优化器进行超参数搜索，hyperopt优化器也是贝叶斯优化器的一种，可以进行连续变量和离散变量的搜索，目前支持的搜索算法包括随机搜索（random search）、模拟退火（simulated annealing）和TPE（Tree of Parzen Estimator）算法，相比网格搜索，hyperopt效率更快、精度更高。首次使用hyperopt库可使用pip进行安装：\n",
    "\n",
    "同时，在本次建模中，我们也将采用wrapper方法进行特征筛选，即根据模型输出结果来进行特征筛选，由于很多时候相关系数并不能很好的衡量特征实际对于标签的重要性，因此wrapper筛选的特征往往更加有效。当然，如果希望我们特征筛选结果更加具有可信度，则可以配合交叉验证过程对其进行筛选。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "#其中hp是参数空间创建函数，fmin是参数搜索函数，tpe则是一种基于贝叶斯过程的搜索策略。\n",
    "from hyperopt import hp, fmin, tpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Wrapper特征筛选\n",
    "\n",
    "接下来是特征筛选过程，此处先择使用Wrapper方法进行特征筛选，通过带入全部数据训练一个LightGBM模型，然后通过观察特征重要性，选取最重要的300个特征。当然，为了进一步确保挑选过程的有效性，此处我们考虑使用交叉验证的方法来进行多轮验证。实际多轮验证特征重要性的过程也较为清晰，我们只需要记录每一轮特征重要性，并在最后进行简单汇总即可。我们可以通过定义如下函数完成该过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_wrapper(train, test):\n",
    "    \"\"\"\n",
    "    lgm特征重要性筛选函数\n",
    "    :param train:训练数据集\n",
    "    :param test:测试数据集\n",
    "    :return:特征筛选后的训练集和测试集\n",
    "    \"\"\"\n",
    "    \n",
    "    # Part 1.划分特征名称，删除ID列和标签列\n",
    "    print('feature_select_wrapper...')\n",
    "    label = 'target'\n",
    "    features = train.columns.tolist()\n",
    "    features.remove('card_id')\n",
    "    features.remove('target')\n",
    "\n",
    "    # Step 2.配置lgb参数\n",
    "    # 模型参数\n",
    "    params_initial = {\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.1,\n",
    "        'boosting': 'gbdt',\n",
    "        'min_child_samples': 20,\n",
    "        'bagging_seed': 2020,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 1,\n",
    "        'feature_fraction': 0.7,\n",
    "        'max_depth': -1,\n",
    "        'metric': 'rmse',\n",
    "        'reg_alpha': 0,\n",
    "        'reg_lambda': 1,\n",
    "        'objective': 'regression'\n",
    "    }\n",
    "    # 控制参数\n",
    "    # 提前验证迭代效果或停止\n",
    "    ESR = 30\n",
    "    # 迭代次数\n",
    "    NBR = 10000\n",
    "    # 打印间隔\n",
    "    VBE = 50\n",
    "    \n",
    "    # Part 3.交叉验证过程\n",
    "    # 实例化评估器\n",
    "    kf = KFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "    # 创建空容器\n",
    "    fse = pd.Series(0, index=features)\n",
    "    \n",
    "    for train_part_index, eval_index in kf.split(train[features], train[label]):\n",
    "        # 封装训练数据集\n",
    "        train_part = lgb.Dataset(train[features].loc[train_part_index],\n",
    "                                 train[label].loc[train_part_index])\n",
    "        # 封装验证数据集\n",
    "        eval = lgb.Dataset(train[features].loc[eval_index],\n",
    "                           train[label].loc[eval_index])\n",
    "        # 在训练集上进行训练，并同时进行验证\n",
    "        bst = lgb.train(params_initial, train_part, num_boost_round=NBR,\n",
    "                        valid_sets=[train_part, eval],\n",
    "                        valid_names=['train', 'valid'],\n",
    "                        early_stopping_rounds=ESR, verbose_eval=VBE)\n",
    "        # 输出特征重要性计算结果，并进行累加,bst.feature_importance()输出的结果是一维数组列表\n",
    "        return bst.feature_importance()\n",
    "        fse += pd.Series(bst.feature_importance(), index=features)\n",
    "    \n",
    "    # Part 4.选择最重要的300个特征\n",
    "    feature_select = ['card_id'] + fse.sort_values(ascending=False).index.tolist()[:300]\n",
    "    print('done')\n",
    "    return train[feature_select + ['target']], test[feature_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_select_wrapper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "E:\\anaconda\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.435004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 227016\n",
      "[LightGBM] [Info] Number of data points in the train set: 161533, number of used features: 1626\n",
      "[LightGBM] [Info] Start training from score -0.390986\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.43981\tvalid's rmse: 3.69376\n",
      "[100]\ttrain's rmse: 3.32154\tvalid's rmse: 3.69286\n",
      "Early stopping, best iteration is:\n",
      "[108]\ttrain's rmse: 3.30267\tvalid's rmse: 3.69172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([20,  3,  0, ...,  4,  1,  1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_LGBM, test_LGBM = feature_select_wrapper(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_active_month                   20\n",
       "feature_1                             3\n",
       "feature_2                             0\n",
       "feature_3                             2\n",
       "authorized_flag&1&purchase_amount     4\n",
       "                                     ..\n",
       "merchant_id_nunique                   6\n",
       "state_id_nunique                      0\n",
       "subsector_id_nunique                  4\n",
       "card_id_size                          1\n",
       "card_id_count                         1\n",
       "Length: 1740, dtype: int32"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = train.columns.tolist()\n",
    "features.remove('card_id')\n",
    "features.remove('target')\n",
    "pd.Series(result,index=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.LightGBM模型训练与TPE参数优化\n",
    "\n",
    "接下来，我们进行LightGBM的模型训练过程，和此前的随机森林建模过程类似，我们需要在训练模型的过程同时进行超参数的搜索调优。为了能够更好的借助hyperopt进行超参数搜索，此处我们考虑使用LightGBM的原生算法库进行建模，并将整个算法建模流程封装在若干个函数 内执行。\n",
    "\n",
    "- 参数回调函数\n",
    "\n",
    "首先对于lgb模型来说，并不是所有的超参数都需要进行搜索，为了防止多次实例化模型过程中部分超参数被设置成默认参数，此处我们首先需要创建一个参数回调函数，用于在后续多次实例化模型过程中反复申明这部分参数的固定取值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_append(params):\n",
    "    \"\"\"\n",
    "    动态回调参数函数，params视作字典\n",
    "    :param params:lgb参数字典\n",
    "    :return params:修正后的lgb参数字典\n",
    "    \"\"\"\n",
    "    params['feature_pre_filter'] = False\n",
    "    params['objective'] = 'regression'\n",
    "    params['metric'] = 'rmse'\n",
    "    params['bagging_seed'] = 2020\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型训练与参数优化函数\n",
    "\n",
    "接下来就是更加复杂的模型训练与超参数调优的的过程。不同于sklearn内部的调参过程，此处由于涉及多个不同的库相互协同，外加本身lgb模型参数就较为复杂，因此整体模型训练与优化过程较为复杂，我们可以通过下述函数来执行该过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_hyperopt(train):\n",
    "    \"\"\"\n",
    "    模型参数搜索与优化函数\n",
    "    :param train:训练数据集\n",
    "    :return params_best:lgb最优参数\n",
    "    \"\"\"\n",
    "    # Part 1.划分特征名称，删除ID列和标签列\n",
    "    label = 'target'\n",
    "    features = train.columns.tolist()\n",
    "    features.remove('card_id')\n",
    "    features.remove('target')\n",
    "    \n",
    "    # Part 2.封装训练数据\n",
    "    train_data = lgb.Dataset(train[features], train[label])\n",
    "    \n",
    "    # Part 3.内部函数，输入模型超参数损失值输出函数\n",
    "    params = {}\n",
    "    def hyperopt_objective(params):\n",
    "        \"\"\"\n",
    "        输入超参数，输出对应损失值\n",
    "        :param params:\n",
    "        :return:最小rmse\n",
    "        \"\"\"\n",
    "        # 创建参数集\n",
    "        params = params_append(params)\n",
    "        print(params)\n",
    "        \n",
    "        # 借助lgb的cv过程，输出某一组超参数下损失值的最小值\n",
    "        res = lgb.cv(params, train_data, 1000,\n",
    "                     nfold=2,\n",
    "                     stratified=False,\n",
    "                     shuffle=True,\n",
    "                     metrics='rmse',\n",
    "                     early_stopping_rounds=20,\n",
    "                     verbose_eval=False,\n",
    "                     show_stdv=False,\n",
    "                     seed=2020)\n",
    "        return min(res['rmse-mean']) # res是个字典\n",
    "\n",
    "    # Part 4.lgb超参数空间\n",
    "    params_space = {\n",
    "        'learning_rate': hp.uniform('learning_rate', 1e-2, 5e-1),\n",
    "        'bagging_fraction': hp.uniform('bagging_fraction', 0.5, 1),\n",
    "        'feature_fraction': hp.uniform('feature_fraction', 0.5, 1),\n",
    "        'num_leaves': hp.choice('num_leaves', list(range(10, 300, 10))),\n",
    "        'reg_alpha': hp.randint('reg_alpha', 0, 10),\n",
    "        'reg_lambda': hp.uniform('reg_lambda', 0, 10),\n",
    "        'bagging_freq': hp.randint('bagging_freq', 1, 10),\n",
    "        'min_child_samples': hp.choice('min_child_samples', list(range(1, 30, 5)))\n",
    "    }\n",
    "    \n",
    "    # Part 5.TPE超参数搜索\n",
    "    params_best = fmin(\n",
    "        hyperopt_objective,\n",
    "        space=params_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=30,\n",
    "        rstate=RandomState(2020))\n",
    "    \n",
    "    # 返回最佳参数\n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#带入训练集数据，此时best_clf即为lgb模型的最优参数组。\n",
    "best_clf = param_hyperopt(train_LGBM)\n",
    "best_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.LightGBM模型预测与结果排名\n",
    "\n",
    "在搜索出最优参数后，接下来即可进行模型预测了。和此前一样，在实际执行预测时有两种思路，其一是单模型预测，即直接针对测试集进行预测并提交结果，其二则是通过交叉验证提交平均得分，并且在此过程中能同时保留下后续用于stacking集成时所需要用到的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 单模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再次申明固定参数\n",
    "best_clf = params_append(best_clf)\n",
    "\n",
    "# 数据准备过程\n",
    "label = 'target'\n",
    "features = train_LGBM.columns.tolist()\n",
    "features.remove('card_id')\n",
    "features.remove('target')\n",
    "\n",
    "# 数据封装\n",
    "lgb_train = lgb.Dataset(train_LGBM[features], train_LGBM[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在全部数据集上训练模型\n",
    "bst = lgb.train(best_clf, lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在测试集上完成预测\n",
    "bst.predict(train_LGBM[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单查看训练集RMSE\n",
    "np.sqrt(mean_squared_error(train_LGBM[label], bst.predict(train_LGBM[features])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#进行预测，并写入本地文件\n",
    "test_LGBM['target'] = bst.predict(test_LGBM[features])\n",
    "test_LGBM[['card_id', 'target']].to_csv(path+\"submission_LGBM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 结合交叉验证进行模型预测\n",
    "\n",
    "和随机森林借助交叉验证进行模型预测的过程类似，lgb也需要遵照如下流程进行训练和预测，并同时创建后续集成所需数据集以及预测结果的平均值（作为最终预测结果）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/08/ALF3cfuSwmB7b8z.png\" alt=\"image-20211208192640281\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(train, test, params):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train:\n",
    "    :param test:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Part 1.选择特征\n",
    "    label = 'target'\n",
    "    features = train.columns.tolist()\n",
    "    features.remove('card_id')\n",
    "    features.remove('target')\n",
    "    \n",
    "    # Part 2.再次申明固定参数与控制迭代参数\n",
    "    params = params_append(params)\n",
    "    ESR = 30\n",
    "    NBR = 10000\n",
    "    VBE = 50\n",
    "    \n",
    "    # Part 3.创建结果存储容器\n",
    "    # 测试集预测结果存储器，后保存至本地文件\n",
    "    prediction_test = 0\n",
    "    # 验证集的模型表现，作为展示用\n",
    "    cv_score = []\n",
    "    # 验证集的预测结果存储器，后保存至本地文件\n",
    "    prediction_train = pd.Series()\n",
    "    \n",
    "    # Part 3.交叉验证\n",
    "    kf = KFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "    for train_part_index, eval_index in kf.split(train[features], train[label]):\n",
    "        # 训练数据封装\n",
    "        train_part = lgb.Dataset(train[features].loc[train_part_index],\n",
    "                                 train[label].loc[train_part_index])\n",
    "        # 测试数据封装\n",
    "        eval = lgb.Dataset(train[features].loc[eval_index],\n",
    "                           train[label].loc[eval_index])\n",
    "        # 依据验证集训练模型\n",
    "        bst = lgb.train(params, train_part, num_boost_round=NBR,\n",
    "                        valid_sets=[train_part, eval],\n",
    "                        valid_names=['train', 'valid'],\n",
    "                        early_stopping_rounds=ESR, verbose_eval=VBE)\n",
    "        # 测试集预测结果并纳入prediction_test容器\n",
    "        prediction_test += bst.predict(test[features])\n",
    "        # 验证集预测结果并纳入prediction_train容器\n",
    "        prediction_train = prediction_train.append(pd.Series(bst.predict(train[features].loc[eval_index]),\n",
    "                                                             index=eval_index))\n",
    "        # 验证集预测结果\n",
    "        eval_pre = bst.predict(train[features].loc[eval_index])\n",
    "        # 计算验证集上得分\n",
    "        score = np.sqrt(mean_squared_error(train[label].loc[eval_index].values, eval_pre))\n",
    "        # 纳入cv_score容器\n",
    "        cv_score.append(score)\n",
    "        \n",
    "    # Part 4.打印/输出结果\n",
    "    # 打印验证集得分与平均得分\n",
    "    print(cv_score, sum(cv_score) / 5)\n",
    "    # 将验证集上预测结果写入本地文件\n",
    "    pd.Series(prediction_train.sort_index().values).to_csv(path + \"train_lightgbm.csv\", index=False)\n",
    "    # 将测试集上预测结果写入本地文件\n",
    "    pd.Series(prediction_test / 5).to_csv(path + \"test_lightgbm.csv\", index=False)\n",
    "    # 测试集平均得分作为模型最终预测结果\n",
    "    test['target'] = prediction_test / 5\n",
    "    # 将测试集预测结果写成竞赛要求格式并保存至本地\n",
    "    test[['card_id', 'target']].to_csv(path + \"submission_lightgbm.csv\", index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LGBM, test_LGBM = feature_select_wrapper(train, test)\n",
    "best_clf = param_hyperopt(train_LGBM)\n",
    "train_predict(train_LGBM, test_LGBM, best_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> 二、单模训练策略（三）\n",
    "## <center> **NLP特征优化+XGBoost建模+贝叶斯优化器**\n",
    "    \n",
    "进一步优化提升模型效果，可以考虑围绕数据集中的部分ID字段进行NLP特征优化。因此，接下来，我们考虑采用CountVectorizer, TfidfVectorizer两种方法对数据集中部分特征进行NLP特征衍生，并且采用XGBoost模型进行预测，同时考虑进一步使用另一种贝叶斯优化器（bayes_opt）来进行模型参数调优。\n",
    "    \n",
    "    当然，此处训练的三个模型分别采用了不同的优化器、甚至是采用了不同的特征衍生的方法，也是为了令这三个模型尽可能的存在一定的差异性，从而为后续的模型融合的融合效果做铺垫。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.NLP特征优化\n",
    "\n",
    "首先我们注意到，在数据集中存在大量的ID相关的列（除了card_id外），包括'merchant_id'、'merchant_category_id'、'state_id'、'subsector_id'、'city_id'等，考虑到这些ID在出现频率方面都和用户实际的交易行为息息相关，例如对于单独用户A来说，在其交易记录中频繁出现某商户id（假设为B），则说明该用户A对商户B情有独钟，而如果在不同的用户交易数据中，都频繁的出现了商户B，则说明这家商户受到广泛欢迎，而进一步的说明A的喜好可能和大多数用户一致，而反之则说明A用户的喜好较为独特。为了能够挖掘出类似信息，我们可以考虑采用NLP中CountVector和TF-IDF两种方法来进行进一步特征衍生，其中CountVector可以挖掘类似某用户钟爱某商铺的信息，而TF-IDF则可进一步挖掘出类似某用户的喜好是否普遍或一致等信息。\n",
    "\n",
    "此外，若要借助NLP方法进行进一步特征衍生，则需要考虑到新创建的特征数量过大所导致的问题，因此我们建议在使用上述方法的同时，考虑借助scipy中的稀疏矩阵相关方法，来进行新特征的存储与读取。即采用CSR格式来创建稀疏矩阵，用npz格式来进行本地数据文件保存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= 'D:\\数据挖掘实战\\Kaggle实例-3\\案例一【Elo用户忠诚度预测】\\Part 1\\数据\\原始数据\\\\'\n",
    "train = pd.read_csv(path+\"train.csv\")\n",
    "test = pd.read_csv(path+\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意，该数据集是最初始的数据集\n",
    "train = pd.read_csv(path+'train.csv')\n",
    "test =  pd.read_csv(path+'test.csv')\n",
    "merchant = pd.read_csv(path+'merchants.csv')\n",
    "new_transaction = pd.read_csv(path+'new_merchant_transactions.csv')\n",
    "history_transaction = pd.read_csv(path+'historical_transactions.csv')\n",
    "transaction = pd.concat([new_transaction, history_transaction], axis=0, ignore_index=True)\n",
    "del new_transaction\n",
    "del history_transaction\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_features = ['merchant_id', 'merchant_category_id', 'state_id', 'subsector_id', 'city_id']\n",
    "#将上述特征按照card_id聚合成以空格拼接的字符串，便于onehot编码\n",
    "for co in nlp_features:\n",
    "    print(co)\n",
    "    transaction[co] = transaction[co].astype(str)\n",
    "    temp = transaction[transaction['month_lag']>=0].groupby(\"card_id\")[co].apply(list).apply(lambda x:' '.join(x)).reset_index()\n",
    "    temp.columns = ['card_id', co+'_new']\n",
    "    train = pd.merge(train, temp, how='left', on='card_id')\n",
    "    test = pd.merge(test, temp, how='left', on='card_id')\n",
    "\n",
    "    temp = transaction[transaction['month_lag']<0].groupby(\"card_id\")[co].apply(list).apply(lambda x:' '.join(x)).reset_index()\n",
    "    temp.columns = ['card_id', co+'_hist']\n",
    "    train = pd.merge(train, temp, how='left', on='card_id')\n",
    "    test = pd.merge(test, temp, how='left', on='card_id')\n",
    "\n",
    "    temp = transaction.groupby(\"card_id\")[co].apply(list).apply(lambda x:' '.join(x)).reset_index()\n",
    "    temp.columns = ['card_id', co+'_all']\n",
    "    train = pd.merge(train, temp, how='left', on='card_id').fillna(\"-1\")\n",
    "    test = pd.merge(test, temp, how='left', on='card_id').fillna(\"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建空DataFrame用于保存NLP特征\n",
    "train_x = pd.DataFrame()\n",
    "test_x = pd.DataFrame()\n",
    "\n",
    "# 实例化CountVectorizer评估器与TfidfVectorizer评估器\n",
    "cntv = CountVectorizer()\n",
    "tfv = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "\n",
    "# 创建空列表用户保存修正后的列名称\n",
    "vector_feature =[]\n",
    "for co in ['merchant_id', 'merchant_category_id', 'state_id', 'subsector_id', 'city_id']:\n",
    "    vector_feature.extend([co+'_new', co+'_hist', co+'_all'])\n",
    "    \n",
    "# 提取每一列进行新特征衍生\n",
    "for feature in vector_feature:\n",
    "    print(feature)\n",
    "    cntv.fit([feature].append(test[feature]))\n",
    "    train_x = sparse.hstack((train_x, cntv.transform(train[feature]))).tocsr()\n",
    "    test_x = sparse.hstack((test_x, cntv.transform(test[feature]))).tocsr()\n",
    "    \n",
    "    tfv.fit(train[feature].append(test[feature]))\n",
    "    train_x = sparse.hstack((train_x, tfv.transform(train[feature]))).tocsr()\n",
    "    test_x = sparse.hstack((test_x, tfv.transform(test[feature]))).tocsr()\n",
    "    \n",
    "# 保存NLP特征衍生结果\n",
    "sparse.save_npz(path+\"train_nlp.npz\", train_x)\n",
    "sparse.save_npz(path+\"test_nlp.npz\", test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['merchant_id_new',\n",
       " 'merchant_id_hist',\n",
       " 'merchant_id_all',\n",
       " 'merchant_category_id_new',\n",
       " 'merchant_category_id_hist',\n",
       " 'merchant_category_id_all',\n",
       " 'state_id_new',\n",
       " 'state_id_hist',\n",
       " 'state_id_all',\n",
       " 'subsector_id_new',\n",
       " 'subsector_id_hist',\n",
       " 'subsector_id_all',\n",
       " 'city_id_new',\n",
       " 'city_id_hist',\n",
       " 'city_id_all']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_feature =[]\n",
    "for co in ['merchant_id', 'merchant_category_id', 'state_id', 'subsector_id', 'city_id']:\n",
    "    vector_feature.extend([co+'_new', co+'_hist', co+'_all'])\n",
    "vector_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.XGBoost模型训练与优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import f_regression\n",
    "from numpy.random import RandomState\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= 'D:\\数据挖掘实战\\Kaggle实例-3\\案例一【Elo用户忠诚度预测】\\Part 1\\数据\\建模数据\\\\'\n",
    "train = pd.read_csv(path+'train.csv')\n",
    "test = pd.read_csv(path+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train.columns.tolist()\n",
    "features.remove('card_id')\n",
    "features.remove('target')\n",
    "\n",
    "train_x = sparse.load_npz(path+\"train_nlp.npz\")\n",
    "test_x = sparse.load_npz(path+\"test_nlp.npz\")\n",
    "\n",
    "train_x = sparse.hstack((train_x, train[features])).tocsr()\n",
    "test_x = sparse.hstack((test_x, test[features])).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型训练与优化\n",
    "\n",
    "接下来进行模型训练，本轮训练的流程和lgb模型流程类似，首先需要创建用于重复申明固定参数的函数，然后定义搜索和优化函数，并在优化函数内部调用参数回调函数，最后定义模型预测函数，该函数将借助交叉验证过程来进行测试集预测，并同步创建验证集预测结果与每个模型对测试集的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数回调函数\n",
    "def params_append(params):\n",
    "    \"\"\"\n",
    "\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    params['objective'] = 'reg:squarederror'\n",
    "    params['eval_metric'] = 'rmse'\n",
    "    params[\"min_child_weight\"] = int(params[\"min_child_weight\"])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    return params\n",
    "\n",
    "# 模型优化函数\n",
    "def param_beyesian(train):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Part 1.数据准备\n",
    "    train_y = pd.read_csv(\"data/train.csv\")['target']\n",
    "    # 数据封装\n",
    "    sample_index = train_y.sample(frac=0.1, random_state=2020).index.tolist()\n",
    "    train_data = xgb.DMatrix(train.tocsr()[sample_index, :\n",
    "                             ], train_y.loc[sample_index].values, silent=True)\n",
    "    \n",
    "    # 借助cv过程构建目标函数\n",
    "    def xgb_cv(colsample_bytree, subsample, min_child_weight, max_depth,\n",
    "               reg_alpha, eta,\n",
    "               reg_lambda):\n",
    "        \"\"\"\n",
    "\n",
    "        :param colsample_bytree:\n",
    "        :param subsample:\n",
    "        :param min_child_weight:\n",
    "        :param max_depth:\n",
    "        :param reg_alpha:\n",
    "        :param eta:\n",
    "        :param reg_lambda:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        params = {'objective': 'reg:squarederror',\n",
    "                  'early_stopping_round': 50,\n",
    "                  'eval_metric': 'rmse'}\n",
    "        params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "        params['subsample'] = max(min(subsample, 1), 0)\n",
    "        params[\"min_child_weight\"] = int(min_child_weight)\n",
    "        params['max_depth'] = int(max_depth)\n",
    "        params['eta'] = float(eta)\n",
    "        params['reg_alpha'] = max(reg_alpha, 0)\n",
    "        params['reg_lambda'] = max(reg_lambda, 0)\n",
    "        print(params)\n",
    "        cv_result = xgb.cv(params, train_data,\n",
    "                           num_boost_round=1000,\n",
    "                           nfold=2, seed=2,\n",
    "                           stratified=False,\n",
    "                           shuffle=True,\n",
    "                           early_stopping_rounds=30,\n",
    "                           verbose_eval=False)\n",
    "        return -min(cv_result['test-rmse-mean'])\n",
    "    \n",
    "    # 调用贝叶斯优化器进行模型优化\n",
    "    xgb_bo = BayesianOptimization(\n",
    "        xgb_cv,\n",
    "        {'colsample_bytree': (0.5, 1),\n",
    "         'subsample': (0.5, 1),\n",
    "         'min_child_weight': (1, 30),\n",
    "         'max_depth': (5, 12),\n",
    "         'reg_alpha': (0, 5),\n",
    "         'eta':(0.02, 0.2),\n",
    "         'reg_lambda': (0, 5)}\n",
    "    )\n",
    "    xgb_bo.maximize(init_points=21, n_iter=5)  # init_points表示初始点，n_iter代表迭代次数（即采样数）\n",
    "    print(xgb_bo.max['target'], xgb_bo.max['params'])\n",
    "    return xgb_bo.max['params']\n",
    "\n",
    "# 交叉验证预测函数\n",
    "def train_predict(train, test, params):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train:\n",
    "    :param test:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_y = pd.read_csv(\"data/train.csv\")['target']\n",
    "    test_data = xgb.DMatrix(test)\n",
    "\n",
    "    params = params_append(params)\n",
    "    kf = KFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "    prediction_test = 0\n",
    "    cv_score = []\n",
    "    prediction_train = pd.Series()\n",
    "    ESR = 30\n",
    "    NBR = 10000\n",
    "    VBE = 50\n",
    "    for train_part_index, eval_index in kf.split(train, train_y):\n",
    "        # 模型训练\n",
    "        train_part = xgb.DMatrix(train.tocsr()[train_part_index, :],\n",
    "                                 train_y.loc[train_part_index])\n",
    "        eval = xgb.DMatrix(train.tocsr()[eval_index, :],\n",
    "                           train_y.loc[eval_index])\n",
    "        bst = xgb.train(params, train_part, NBR, [(train_part, 'train'),\n",
    "                                                          (eval, 'eval')], verbose_eval=VBE,\n",
    "                        maximize=False, early_stopping_rounds=ESR, )\n",
    "        prediction_test += bst.predict(test_data)\n",
    "        eval_pre = bst.predict(eval)\n",
    "        prediction_train = prediction_train.append(pd.Series(eval_pre, index=eval_index))\n",
    "        score = np.sqrt(mean_squared_error(train_y.loc[eval_index].values, eval_pre))\n",
    "        cv_score.append(score)\n",
    "    print(cv_score, sum(cv_score) / 5)\n",
    "    pd.Series(prediction_train.sort_index().values).to_csv(path+\"train_xgboost.csv\", index=False)\n",
    "    pd.Series(prediction_test / 5).to_csv(path+\"test_xgboost.csv\", index=False)\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "    test['target'] = prediction_test / 5\n",
    "    test[['card_id', 'target']].to_csv(path+\"submission_xgboost.csv\", index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = param_beyesian(train_x)\n",
    "train_predict(train_x, test_x, best_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> 三、模型融合策略(一)\n",
    "## <center> **Voting融合**\n",
    "    \n",
    "    整体来看，常用模型融合的策略有两种，分别是Voting融合与Stacking融合，模型融合的目的和集成模型中的集成过程类似，都是希望能够尽可能借助不同模型的优势，最终输出一个更加可靠的结果。在Voting过程中，我们只需要对不同模型对测试集的预测结果进行加权汇总即可，而Stacking则相对复杂，需要借助此前不同模型的验证集预测结果和测试集预测结果再次进行模型训练，以验证集预测结果为训练集、训练集标签为标签构建新的训练集，在此模型上进行训练，然后以测试集预测结果作为新的预测集，并在新预测集上进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.均值融合\n",
    "\n",
    "首先我们来看Voting融合过程。一般来说Voting融合也可以分为均值融合（多组预测结果求均值）、加权融合（根据某种方式赋予不同预测结果不同权重而后进行求和）以及Trick融合（根据某种特殊的规则赋予权重而后进行求和）三种，此处先介绍均值融合与加权融合的基本过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 拼接不同模型预测结果\n",
    "\n",
    "首先，对随机森林、LightGBM和XGBoost模型预测结果进行读取，并简单查看三者相关系数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"result/submission_randomforest.csv\")\n",
    "data['randomforest'] = data['target'].values\n",
    "\n",
    "temp = pd.read_csv(\"result/submission_lightgbm.csv\")\n",
    "data['lightgbm'] = temp['target'].values\n",
    "\n",
    "\n",
    "temp = pd.read_csv(\"result/submission_xgboost.csv\")\n",
    "data['xgboost'] = temp['target'].values\n",
    "\n",
    "print(data.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-6c0aa5063889>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 然后计算不同模型预测结果的均值：\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'randomforest'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lightgbm'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'xgboost'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'card_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"result/voting_avr.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# 然后计算不同模型预测结果的均值：\n",
    "data['target'] = (data['randomforest'] + data['lightgbm'] + data['xgboost']) / 3\n",
    "data[['card_id','target']].to_csv(\"result/voting_avr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 结果如下\n",
    "\n",
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "\n",
    "能够发现，简单的均值融合并不能有效果上的提升，接下来我们尝试加权融合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.加权融合\n",
    "\n",
    "加权融合的思路并不复杂，从客观计算流程上来看我们将赋予不同模型训练结果以不同权重，而具体权重的分配，我们可以根据三组模型在公榜上的评分决定，即假设模型A和B分别是2分和3分（分数越低越好的情况下），则在实际加权过程中，我们将赋予A模型结果3/5权重，B模型2/5权重，因此，加权融合过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = data['randomforest']*0.3+data['lightgbm']*0.2 + data['xgboost']*0.5\n",
    "data[['card_id','target']].to_csv(\"result/voting_wei1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 结果如下\n",
    "\n",
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "| Voting_wei | 3.633307 | 3.72877 |\n",
    "\n",
    "能够发现结果略微有所改善，但实际结果仍然不如单模型结果，此处预测极有可能是因额外i标签中存在异常值导致。接下来继续尝试Stacking融合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> 四、模型融合策略（二）\n",
    "## <center> **Stacking融合**\n",
    "\n",
    "此处我们考虑手动进行Stacking融合，在此前的模型训练中，我们已经创建了predication_train和predication_test数据集，这两个数据集将作为训练集、测试集带入到下一轮的建模中，而本轮建模也被称为Stacking融合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/08/ALF3cfuSwmB7b8z.png\" alt=\"image-20211208192640281\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据集校验\n",
    "\n",
    "首先快速查看此前创建的相关数据集，其中oof就是训练数据集的预测结果（多轮验证集预测结果拼接而来），而predictions则是此前单模型的预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_rf  = pd.read_csv('./preprocess/train_randomforest.csv')\n",
    "predictions_rf  = pd.read_csv('./preprocess/test_randomforest.csv')\n",
    "\n",
    "oof_lgb  = pd.read_csv('./preprocess/train_lightgbm.csv')\n",
    "predictions_lgb  = pd.read_csv('./preprocess/test_lightgbm.csv')\n",
    "\n",
    "oof_xgb  = pd.read_csv('./preprocess/train_xgboost.csv')\n",
    "predictions_xgb  = pd.read_csv('./preprocess/test_xgboost.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_model(oof_1, oof_2, oof_3, predictions_1, predictions_2, predictions_3, y):\n",
    "   \n",
    "    # Part 1.数据准备\n",
    "    # 按列拼接，拼接验证集所有预测结果\n",
    "    # train_stack就是final model的训练数据\n",
    "    train_stack = np.hstack([oof_1, oof_2, oof_3])\n",
    "    # 按列拼接，拼接测试集上所有预测结果\n",
    "    # test_stack就是final model的测试数据\n",
    "    test_stack = np.hstack([predictions_1, predictions_2, predictions_3])\n",
    "    # 创建一个和验证集行数相同的全零数组\n",
    "    # oof = np.zeros(train_stack.shape[0])\n",
    "    # 创建一个和测试集行数相同的全零数组\n",
    "    predictions = np.zeros(test_stack.shape[0])\n",
    "    \n",
    "    # Part 2.多轮交叉验证\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=2020)\n",
    "    \n",
    "    # fold_为折数，trn_idx为每一折训练集index，val_idx为每一折验证集index\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, y)):\n",
    "        # 打印折数信息\n",
    "        print(\"fold n°{}\".format(fold_+1))\n",
    "        # 训练集中划分为训练数据的特征和标签\n",
    "        trn_data, trn_y = train_stack[trn_idx], y[trn_idx]\n",
    "        # 训练集中划分为验证数据的特征和标签\n",
    "        val_data, val_y = train_stack[val_idx], y[val_idx]\n",
    "        # 开始训练时提示\n",
    "        print(\"-\" * 10 + \"Stacking \" + str(fold_+1) + \"-\" * 10)\n",
    "        # 采用贝叶斯回归作为结果融合的模型（final model）\n",
    "        clf = BayesianRidge()\n",
    "        # 在训练数据上进行训练\n",
    "        clf.fit(trn_data, trn_y)\n",
    "        # 在验证数据上进行预测，并将结果记录在oof对应位置\n",
    "        # oof[val_idx] = clf.predict(val_data)\n",
    "        # 对测试集数据进行预测，每一轮预测结果占比额外的1/10\n",
    "        predictions += clf.predict(test_stack) / (5 * 2)\n",
    "    \n",
    "    # 返回测试集的预测结果\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['target'].values\n",
    "predictions_stack  = stack_model(oof_rf, oof_lgb, oof_xgb, \n",
    "                                 predictions_rf, predictions_lgb, predictions_xgb, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_stack\n",
    "sub_df.to_csv('predictions_stack1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
