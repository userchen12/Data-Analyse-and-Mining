{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>特征优化与模型融合优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm,tqdm_notebook \n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from scipy import sparse\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('max_colwidth',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>一、优化思路梳理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 整体优化思路\n",
    "\n",
    "对于机器学习来说，总的来看有两种建模思路，其一是通过特征工程方法进一步提升数据质量，其二则是通过更加复杂的模型或更加有效的模型融合技巧来提升建模效果，并且就二者的关系来看，正如时下流行的观点所说，特征工程将决定模型效果上界，而建模过程则会不断逼近这个上界。但无论如何，在优化的过程中，需要二者配合执行才能达到更好的效果。\n",
    "\n",
    "<center><img src=\"https://s2.loli.net/2021/12/10/yfdwsnt2okYpxmR.png\" alt=\"image-20211210125340547\" style=\"zoom:20%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.特征优化思路\n",
    "\n",
    "首先，先来看特征优化思路。在此前的建模过程中，我们曾不止一次的对特征进行了处理，首先是在数据聚合时（以card_id进行聚合），为了尽可能提取更多的交易数据信息与商户信息带入进行模型，我们围绕交易数据表和商户数据表进行了工程化批量特征衍生，彼时信息提取流程如下：\n",
    "\n",
    "<center><img src=\"https://s2.loli.net/2021/12/07/9jwd1meblXaoUzh.png\" alt=\"image-20211207235615308\" style=\"zoom:50%;\" />\n",
    "    \n",
    "具体的特征衍生方案是采用了交叉组合特征创建与业务统计特征创建两种方案：\n",
    "    \n",
    "<center><img src=\"https://s2.loli.net/2021/12/08/gsfO4t5cBa7q61p.png\" alt=\"image-20211208002218370\" style=\"zoom:50%;\" />\n",
    "    \n",
    "该过程的详细讲解，可参考Day 3-Day 4的课程内容。总而言之，通过该过程，我们顺利的提取了交易信息表和商户信息表中的数据带入进行建模，并且借助随机森林模型，顺利跑通Baseline。但值得一提的是，在上述流程中，我们其实只是采用了一些工程化的通用做法，这些方法是可以快速适用于任何数据集的特征衍生环节，同时这样的方法也应该是所有建模开始前必须尝试的做法，但既然是“通用”方法，那必然无法帮我们在实际竞赛中脱颖而出。\n",
    "\n",
    "当然，我们也曾尝试过进行有针对性的特征优化，在Day 5的内容中，我们曾采用NLP方法用于提取特征ID列的信息，并得到了一系列能够更加细致描述用户行为信息与商品偏好的特征，借助该特征，我们最终训练得出了一个效果更好的模型，该结果也进一步验证了特征优化对模型效果提升所能起到的作用。接下来我怕们也将尝试进一步进行有针对性的特征优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 结合实际情况进行优化\n",
    "- 用户行为特征\n",
    "\n",
    "首先，我们注意到，每一笔信用卡的交易记录都有交易时间，而对于时间字段和文本字段，普通的批量创建特征的方法都是无法较好的挖掘其全部信息的，因此我们需要围绕交易字段中的交易时间进行额外的特征衍生。此处我们可以考虑构造一些用于描述用户行为习惯的特征（经过反复验证，用户行为特征是最为有效的提高预测结果的特征类），包括最近一次交易与首次交易的时间差、信用卡激活日期与首次交易的时间差、用户两次交易平均时间间隔、按照不同交易地点/商品品类进行聚合（并统计均值、方差等统计量）。      \n",
    "此外，我们也知道越是接近当前时间点的用户行为越有价值，因此我们还需要重点关注用户最近两个月（实际时间跨度可以自行决定）的行为特征，以两个月为跨度，进一步统计该时间周期内用户的上述交易行为特点，并带入模型进行训练。\n",
    "\n",
    "- 二阶交叉特征\n",
    "\n",
    "在此前的特征衍生过程中，我们曾进行了交叉特征衍生，但只是进行了一阶交叉衍生，例如交易额在不同商品上的汇总，但实际上还可以进一步构造二阶衍生，例如交易额在不同商品组合上的汇总。通常来说更高阶的衍生会导致特征矩阵变得更加稀疏，并且由于每一阶的衍生都会创造大量特征，因此更高阶的衍生往往也会造成维度爆炸，因此高阶交叉特征衍生需要谨慎。不过正如此前我们考虑的，由于用户行为特征对模型结果有更大的影响，因此我们可以单独围绕用户行为数据进行二阶交叉特征衍生，并在后续建模前进行特征筛选。\n",
    "\n",
    "- 异常值识别特征\n",
    "\n",
    "在Day 1的数据探索中我们就发现，训练数据集的标签中存在少量极端异常值的情况\n",
    "<center><img src=\"https://s2.loli.net/2021/12/10/neGfhCq5xBcO9uL.png\" alt=\"image-20211210155414409\" style=\"zoom:33%;\" />\n",
    "\n",
    "而若我们更进一步就此前的建模结果展开分析的话，我们会发现，此前的模型误差大多数都源于对异常值用户(card_id)的预测结果。而根据Day 1的讨论我们知道，实际上用户评分是通过某套公式人工计算得出的，因此这些异常值极有可能是某类特殊用户的标记，因此我们不妨在实际建模过程中进行两阶段建模，即先预测每个输入样本是否是异常样本，并根据分类预测结果进行后续的回归建模，基本流程如下：\n",
    "    \n",
    "<center><img src=\"https://s2.loli.net/2021/12/10/PtW9zf6EpchjK4o.png\" alt=\"image-20211210161251760\" style=\"zoom:33%;\" />\n",
    "\n",
    "    而为了保证后续两阶段建模时第一阶段的分类模型能够更加准确的识别异常用户，我们需要创建一些基于异常用户的特征聚合字段，例如异常用户平均消费次数、消费金额等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.模型优化思路\n",
    "\n",
    "- 新增CatBoost模型\n",
    "\n",
    "相比特征优化思路，模型优化的思路相对简单，首先从模型选择来看，相较LightGBM和XGBoost，随机森林对当前数据集的预测能力较弱，可以考虑将其换成集成算法新秀：CatBoost。\n",
    "\n",
    "CatBoost是由俄罗斯搜索引擎Yandex在2017年7月开源的一个GBM算法，自开源之日起，CatBoost就因为其强大的效果与极快的执行效率广受算法工程人员青睐。在诸多CatBoost的算法优势中，最引人关注的当属该模型能够自主采用独热编码和平均编码的混合策略来处理类别特征，也就是说CatBoost将一些经过实践验证的、普遍有效的特征工程方法融入了实际模型训练过程；此外，CatBoost还提出了一种全新的梯度提升的机制，能够非常好的在经验风险和结构风险中做出权衡（即能够很好的提升精度、同时又能够避免过拟合问题）。\n",
    "\n",
    "而在后续的建模环节中，我们就将使用CatBoost替换随机森林，并最终带入CatBoost、XGBoost和LightGBM三个模型进行模型融合。\n",
    "\n",
    "- 二阶段建模\n",
    "\n",
    "而从模型训练流程角度出发，则可以考虑进行二阶段建模，基本流程如下：\n",
    "\n",
    "<center><img src=\"https://s2.loli.net/2021/12/10/ioPh1C5uzVgE9dZ.png\" alt=\"image-20211210164131623\" style=\"zoom:33%;\" />\n",
    "\n",
    "    并且，需要注意的是，在实际二阶段建模过程时，我们需要在每个建模阶段都进行交叉验证与模型融合，才能最大化提升模型效果。也就是说我们需要训练三组模型（以及对应进行三次模型融合），来完成分类预测问题、普通用户回归预测问题和异常用户回归预测问题。三轮建模关系如下：\n",
    "    \n",
    "<center><img src=\"https://s2.loli.net/2021/12/10/a19mJpQOkMunDbf.png\" alt=\"image-20211210165529875\" style=\"zoom:33%;\" />  \n",
    "    \n",
    "    不难发现，整体建模与融合过程都将变得更加复杂。不过这也是更加贴近真实状态的一种情况，很多时候算法和融合过程都只是元素，如何构建一个更加精准、高效的训练流程，才是进阶的算法工程人员更需要考虑的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.其他注意事项\n",
    "\n",
    "- 内存管理\n",
    "\n",
    "由于接下来我们需要反复读取数据文件并进行计算，因此需要时刻注意进行内存管理，除了可以通过及时删除不用的变量并使用动态垃圾回收机制来清理内存外，还可以使用如下方式在定义数据类型时尽可能在不影响数值运算的前提下给出更加合适的数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "path = 'D:\\数据挖掘实战\\Kaggle实例-3\\案例一【Elo用户忠诚度预测】\\Part 1\\数据\\原始数据\\\\'\n",
    "new_transactions = pd.read_csv(path+'new_merchant_transactions.csv', parse_dates=['purchase_date'])\n",
    "historical_transactions = pd.read_csv(path+'historical_transactions.csv', parse_dates=['purchase_date'])\n",
    "for col in ['authorized_flag', 'category_1']:\n",
    "    historical_transactions[col] = historical_transactions[col].map({'Y':1, 'N':0})\n",
    "    new_transactions[col]        = new_transactions[col].map({'Y':1, 'N':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 代码管理\n",
    "\n",
    "本部分代码量较大，尽管课上是通过Notebook进行展示，但如果在实际建模过程时，更建议通过自定义模块来存储和调用大段代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>二、数据预处理与特征优化\n",
    "   \n",
    "### 1.数据预处理\n",
    "    \n",
    "    此处由于需要进行特征优化，因此数据处理工作需要从头开始执行。首先需要进行数据读取，并进行缺失值处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:14: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import datetime\n",
    "## 加载训练集，测试集，基本处理\n",
    "path = 'D:\\数据挖掘实战\\Kaggle实例-3\\案例一【Elo用户忠诚度预测】\\Part 1\\数据\\原始数据\\\\'\n",
    "train = pd.read_csv(path+'train.csv')\n",
    "test = pd.read_csv(path+'test.csv')\n",
    "\n",
    "target = train['target']\n",
    "for df in [train, test]:    \n",
    "    df['year']  = df['first_active_month'].fillna('0-0').apply(lambda x:int(str(x).split('-')[0]))\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['elapsed_time'] = (datetime.date(2018,3, 1) - df['first_active_month'].dt.isocalendar().date).dt.isocalendar().days\n",
    "    \n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['weekofyear'] = df['first_active_month'].dt.isocalendar().weekofyear\n",
    "    df['dayofyear'] = df['first_active_month'].dt.isocalendar().dayofyear\n",
    "    df['month'] = df['first_active_month'].dt.isocalendar().month\n",
    "    \n",
    "## 交易表合并train test\n",
    "train_test = pd.concat([train[['card_id','first_active_month']], test[['card_id','first_active_month']] ], axis=0, ignore_index=True)\n",
    "historical_transactions   = historical_transactions.merge(train_test[['card_id','first_active_month']], on=['card_id'], how='left')\n",
    "new_transactions = new_transactions.merge(train_test[['card_id','first_active_month']], on=['card_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后需要进行时间字段的更细粒度的呈现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def month_trans(x): \n",
    "    return x // 30\n",
    "\n",
    "def week_trans(x): \n",
    "    return x // 7\n",
    "\n",
    "## 交易表预处理\n",
    "def get_expand_common(df_):\n",
    "    df = df_.copy()\n",
    "    \n",
    "    df['category_2'].fillna(1.0,inplace=True)\n",
    "    # 此处处理的有些简单，直接将缺失值填充为A了，实际上可以用其他特征生成此处的标记？\n",
    "    df['category_3'].fillna('A',inplace=True)\n",
    "    df['category_3'] = df['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    df['installments'].replace(999, np.nan,inplace=True)\n",
    "    df['installments'].replace(0, 1,inplace=True)\n",
    "    \n",
    "    df['purchase_amount'] = np.round(df['purchase_amount'] / 0.00150265118 + 497.06,8)\n",
    "    df['purchase_amount'] = df.purchase_amount.apply(lambda x: np.round(x))\n",
    "    \n",
    "    df['purchase_date']          =  pd.to_datetime(df['purchase_date']) \n",
    "    df['first_active_month']     =  pd.to_datetime(df['first_active_month']) \n",
    "    df['purchase_hour']          =  df['purchase_date'].dt.hour\n",
    "    df['year']                   = df['purchase_date'].dt.year\n",
    "    df['month']                  =  df['purchase_date'].dt.month\n",
    "    df['day']                    = df['purchase_date'].dt.day\n",
    "    df['hour']                   = df['purchase_date'].dt.hour\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['dayofweek']              =  df['purchase_date'].dt.dayofweek\n",
    "    df['weekend']                =  (df.purchase_date.dt.weekday >=5).astype(int) \n",
    "    df                           =  df.sort_values(['card_id','purchase_date']) \n",
    "    df['purchase_date_floorday'] =  df['purchase_date'].dt.floor('d')  #删除小于day的时间\n",
    "    \n",
    "    # 距离激活时间的相对时间,0, 1,2,3,...,max-act\n",
    "    df['purchase_day_since_active_day']   = df['purchase_date_floorday'] - df['first_active_month']  #ht_card_id_gp['purchase_date_floorday'].transform('min')\n",
    "    df['purchase_day_since_active_day']   = df['purchase_day_since_active_day'].dt.days  #.astype('timedelta64[D]') \n",
    "    df['purchase_month_since_active_day'] = df['purchase_day_since_active_day'].agg(month_trans).values\n",
    "    df['purchase_week_since_active_day']  = df['purchase_day_since_active_day'].agg(week_trans).values\n",
    "    \n",
    "    # 距离最后一天时间的相对时间,0,1,2,3,...,max-act\n",
    "    ht_card_id_gp = df.groupby('card_id')\n",
    "    df['purchase_day_since_reference_day']   =  ht_card_id_gp['purchase_date_floorday'].transform('max') - df['purchase_date_floorday']\n",
    "    df['purchase_day_since_reference_day']   =  df['purchase_day_since_reference_day'].dt.days\n",
    "    # 一个粗粒度的特征(距离最近购买过去了几周，几月)\n",
    "    df['purchase_week_since_reference_day']  = df['purchase_day_since_reference_day'].agg(week_trans).values\n",
    "    df['purchase_month_since_reference_day'] = df['purchase_day_since_reference_day'].agg(month_trans).values\n",
    "    \n",
    "    df['purchase_day_diff']   =  df['purchase_date_floorday'].shift()\n",
    "    df['purchase_day_diff']   =  df['purchase_date_floorday'].values - df['purchase_day_diff'].values\n",
    "    df['purchase_day_diff']   =  df['purchase_day_diff'].dt.days\n",
    "    df['purchase_week_diff']  =  df['purchase_day_diff'].agg(week_trans).values\n",
    "    df['purchase_month_diff'] =  df['purchase_day_diff'].agg(month_trans).values \n",
    "    \n",
    "    df['purchase_amount_ddgd_98']  = df['purchase_amount'].values * df['purchase_day_since_reference_day'].apply(lambda x:0.98**x).values\n",
    "    df['purchase_amount_ddgd_99']  = df['purchase_amount'].values * df['purchase_day_since_reference_day'].apply(lambda x:0.99**x).values    \n",
    "    df['purchase_amount_wdgd_96']  = df['purchase_amount'].values * df['purchase_week_since_reference_day'].apply(lambda x:0.96**x).values \n",
    "    df['purchase_amount_wdgd_97']  = df['purchase_amount'].values * df['purchase_week_since_reference_day'].apply(lambda x:0.97**x).values \n",
    "    df['purchase_amount_mdgd_90']  = df['purchase_amount'].values * df['purchase_month_since_reference_day'].apply(lambda x:0.9**x).values\n",
    "    df['purchase_amount_mdgd_80']  = df['purchase_amount'].values * df['purchase_month_since_reference_day'].apply(lambda x:0.8**x).values \n",
    "    \n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "historical_transactions = get_expand_common(historical_transactions)\n",
    "new_transactions        = get_expand_common(new_transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.特征优化部分\n",
    "\n",
    "在执行完数据清洗与时间字段的处理之后，接下来我们需要开始执行特征优化。根据此前介绍的思路，首先我们需要进行基础行为特征字段衍生："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## 构造基本统计特征\n",
    "def aggregate_transactions(df_, prefix): \n",
    "    \n",
    "    df = df_.copy()\n",
    "    \n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] = df['month_diff'].astype(int)\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    df['price'] = df['purchase_amount'] / df['installments']\n",
    "    df['duration'] = df['purchase_amount'] * df['month_diff']\n",
    "    df['amount_month_ratio'] = df['purchase_amount'] / df['month_diff']\n",
    "    \n",
    "    df.loc[:, 'purchase_date'] = pd.DatetimeIndex(df['purchase_date']).\\\n",
    "                                      astype(np.int64) * 1e-9\n",
    "    \n",
    "    agg_func = {\n",
    "        'category_1':      ['mean'],\n",
    "        'category_2':      ['mean'],\n",
    "        'category_3':      ['mean'],\n",
    "        'installments':    ['mean', 'max', 'min', 'std'],\n",
    "        'month_lag':       ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'month':           ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'hour':            ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'weekofyear':      ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'dayofweek':       ['nunique', 'mean'],\n",
    "        'weekend':         ['mean'],\n",
    "        'year':            ['nunique'],\n",
    "        'card_id':         ['size','count'],\n",
    "        'purchase_date':   ['max', 'min'],\n",
    "        ###\n",
    "        'price':             ['mean','max','min','std'],\n",
    "        'duration':          ['mean','min','max','std','skew'],\n",
    "        'amount_month_ratio':['mean','min','max','std','skew'],\n",
    "        } \n",
    "    \n",
    "    for col in ['category_2','category_3']:\n",
    "        df[col+'_mean'] = df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        agg_func[col+'_mean'] = ['mean']\n",
    "    \n",
    "    agg_df = df.groupby(['card_id']).agg(agg_func)\n",
    "    agg_df.columns = [prefix + '_'.join(col).strip() for col in agg_df.columns.values]\n",
    "    agg_df.reset_index(drop=False, inplace=True)\n",
    "  \n",
    "    return agg_df\n",
    "print('generate statistics features...')\n",
    "auth_base_stat = aggregate_transactions(historical_transactions[historical_transactions['authorized_flag']==1], prefix='auth_')\n",
    "print('generate statistics features...')\n",
    "hist_base_stat = aggregate_transactions(historical_transactions[historical_transactions['authorized_flag']==0], prefix='hist_')\n",
    "print('generate statistics features...')\n",
    "new_base_stat  = aggregate_transactions(new_transactions, prefix='new_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def get_quantile(x, percentiles = [0.1, 0.25, 0.75, 0.9]):\n",
    "    x_len = len(x)\n",
    "    x = np.sort(x)\n",
    "    sts_feas = []  \n",
    "    for per_ in percentiles:\n",
    "        if per_ == 1:\n",
    "            sts_feas.append(x[x_len - 1]) \n",
    "        else:\n",
    "            sts_feas.append(x[int(x_len * per_)]) \n",
    "    return sts_feas \n",
    "\n",
    "def get_cardf_tran(df_, month = 3, prefix = '_'):\n",
    "    \n",
    "    df = df_.copy() \n",
    "    if prefix == 'hist_cardf_':\n",
    "        df['month_to_now']  =  (datetime.date(2018, month, 1) - df['purchase_date_floorday'].dt.date).dt.days\n",
    "    \n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] = df['month_diff'].astype(int)\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    print('*'*30,'Part1, whole data','*'*30)\n",
    "    cardid_features = pd.DataFrame()\n",
    "    cardid_features['card_id'] = df['card_id'].unique()   \n",
    "    print( '*' * 30, 'Traditional Features', '*' * 30)\n",
    "    ht_card_id_gp = df.groupby('card_id') \n",
    "    cardid_features['card_id_cnt'] = ht_card_id_gp['authorized_flag'].count().values\n",
    "    \n",
    "    if  prefix == 'hist_cardf_':\n",
    "        cardid_features['card_id_isau_mean'] = ht_card_id_gp['authorized_flag'].mean().values\n",
    "        cardid_features['card_id_isau_sum'] = ht_card_id_gp['authorized_flag'].sum().values \n",
    "    \n",
    "    cardid_features['month_diff_mean']   = ht_card_id_gp['month_diff'].mean().values\n",
    "    cardid_features['month_diff_median'] = ht_card_id_gp['month_diff'].median().values\n",
    "    \n",
    "    if prefix == 'hist_cardf_':\n",
    "        cardid_features['reference_day']           =  ht_card_id_gp['purchase_date_floorday'].max().values\n",
    "        cardid_features['first_day']               =  ht_card_id_gp['purchase_date_floorday'].min().values \n",
    "        cardid_features['activation_day']          =  ht_card_id_gp['first_active_month'].max().values\n",
    "       \n",
    "        # first to activation day\n",
    "        cardid_features['first_to_activation_day']  =  (cardid_features['first_day'] - cardid_features['activation_day']).dt.days\n",
    "        # activation to reference day \n",
    "        cardid_features['activation_to_reference_day']  =  (cardid_features['reference_day'] - cardid_features['activation_day']).dt.days\n",
    "        # first to last day \n",
    "        cardid_features['first_to_reference_day']  =  (cardid_features['reference_day'] - cardid_features['first_day']).dt.days\n",
    "        # reference day to now  \n",
    "        cardid_features['reference_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['reference_day'].dt.date).dt.days \n",
    "        # first day to now\n",
    "        cardid_features['first_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['first_day'].dt.date).dt.days \n",
    "        \n",
    "        print('card_id(month_lag, min to reference day):min')\n",
    "        cardid_features['card_id_month_lag_min'] = ht_card_id_gp['month_lag'].agg('min').values   \n",
    "        # is_purchase_before_activation,first_to_reference_day_divide_activation_to_reference_day\n",
    "        cardid_features['is_purchase_before_activation'] = cardid_features['first_to_activation_day'] < 0 \n",
    "        cardid_features['is_purchase_before_activation'] = cardid_features['is_purchase_before_activation'].astype(int)\n",
    "        cardid_features['first_to_reference_day_divide_activation_to_reference_day'] = cardid_features['first_to_reference_day']  / (cardid_features['activation_to_reference_day']  + 0.01)\n",
    "        cardid_features['days_per_count'] = cardid_features['first_to_reference_day'].values / cardid_features['card_id_cnt'].values\n",
    "   \n",
    "    if prefix == 'new_cardf_':\n",
    "        print(' Eight time features, ') \n",
    "        cardid_features['reference_day']           =  ht_card_id_gp['reference_day'].last().values\n",
    "        cardid_features['first_day']               =  ht_card_id_gp['purchase_date_floorday'].min().values \n",
    "        cardid_features['last_day']                =  ht_card_id_gp['purchase_date_floorday'].max().values\n",
    "        cardid_features['activation_day']          =  ht_card_id_gp['first_active_month'].max().values\n",
    "        # reference to first day\n",
    "        cardid_features['reference_day_to_first_day']  =  (cardid_features['first_day'] - cardid_features['reference_day']).dt.days\n",
    "        # reference to last day\n",
    "        cardid_features['reference_day_to_last_day']  =  (cardid_features['last_day'] - cardid_features['reference_day']).dt.days  \n",
    "        # first to last day \n",
    "        cardid_features['first_to_last_day']  =  (cardid_features['last_day'] - cardid_features['first_day']).dt.days\n",
    "        # activation to first day \n",
    "        cardid_features['activation_to_first_day']  =  (cardid_features['first_day'] - cardid_features['activation_day']).dt.days\n",
    "        # activation to first day \n",
    "        cardid_features['activation_to_last_day']  =  (cardid_features['last_day'] - cardid_features['activation_day']).dt.days\n",
    "        # last day to now  \n",
    "        cardid_features['reference_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['reference_day'].dt.date).dt.days \n",
    "        # first day to now\n",
    "        cardid_features['first_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['first_day'].dt.date).dt.days \n",
    "        \n",
    "        print('card_id(month_lag, min to reference day):min')\n",
    "        cardid_features['card_id_month_lag_max'] = ht_card_id_gp['month_lag'].agg('max').values  \n",
    "        cardid_features['first_to_last_day_divide_reference_to_last_day'] = cardid_features['first_to_last_day']  / (cardid_features['reference_day_to_last_day']  + 0.01)\n",
    "        cardid_features['days_per_count'] = cardid_features['first_to_last_day'].values / cardid_features['card_id_cnt'].values\n",
    "    \n",
    "    for f in ['reference_day', 'first_day', 'last_day', 'activation_day']:\n",
    "        try:\n",
    "            del cardid_features[f]\n",
    "        except:\n",
    "            print(f, '不存在！！！')\n",
    "\n",
    "    print('card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique') \n",
    "    for col in tqdm_notebook(['category_1','category_2','category_3','state_id','city_id','installments','merchant_id', 'merchant_category_id','subsector_id','month_lag','purchase_date_floorday']):\n",
    "        cardid_features['card_id_%s_nunique'%col]            =  ht_card_id_gp[col].nunique().values\n",
    "        cardid_features['card_id_cnt_divide_%s_nunique'%col] =  cardid_features['card_id_cnt'].values / cardid_features['card_id_%s_nunique'%col].values\n",
    "         \n",
    "    print('card_id(purchase_amount & degrade version ):mean,sum,std,median,quantile(10,25,75,90)') \n",
    "    for col in tqdm_notebook(['installments','purchase_amount','purchase_amount_ddgd_98','purchase_amount_ddgd_99','purchase_amount_wdgd_96','purchase_amount_wdgd_97','purchase_amount_mdgd_90','purchase_amount_mdgd_80']):\n",
    "        if col =='purchase_amount':\n",
    "            for opt in ['sum','mean','std','median','max','min']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values\n",
    "            \n",
    "            cardid_features['card_id_' +col+ '_range'] =  cardid_features['card_id_' +col+ '_max'].values - cardid_features['card_id_' +col+ '_min'].values\n",
    "            percentiles = ht_card_id_gp[col].apply(lambda x:get_quantile(x,percentiles = [0.025, 0.25, 0.75, 0.975])) \n",
    "\n",
    "            cardid_features[col + '_2.5_quantile']  = percentiles.map(lambda x:x[0]).values\n",
    "            cardid_features[col + '_25_quantile'] = percentiles.map(lambda x:x[1]).values\n",
    "            cardid_features[col + '_75_quantile'] = percentiles.map(lambda x:x[2]).values\n",
    "            cardid_features[col + '_97.5_quantile'] = percentiles.map(lambda x:x[3]).values\n",
    "            cardid_features['card_id_' +col+ '_range2'] =  cardid_features[col+ '_97.5_quantile'].values - cardid_features[col+ '_2.5_quantile'].values\n",
    "            del cardid_features[col + '_2.5_quantile'],cardid_features[col + '_97.5_quantile']\n",
    "            gc.collect()\n",
    "        else:\n",
    "            for opt in ['sum']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values          \n",
    "    \n",
    "    print( '*' * 30, 'Pivot Features', '*' * 30)\n",
    "    print('Count  Pivot') #purchase_month_since_reference_day(可能和month_lag重复),百分比降分,暂时忽略 (dayofweek,merchant_cate,state_id)作用不大installments\n",
    "    for pivot_col in tqdm_notebook(['category_1','category_2','category_3','month_lag','subsector_id','weekend']): #'city_id',,\n",
    "    \n",
    "        tmp     = df.groupby(['card_id',pivot_col])['merchant_id'].count().to_frame(pivot_col + '_count')\n",
    "        tmp.reset_index(inplace =True)  \n",
    "         \n",
    "        tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_count',fill_value=0)\n",
    "        tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_cnt_pivot_'+ str(col) for col in tmp_pivot.columns]\n",
    "        tmp_pivot.reset_index(inplace = True)\n",
    "        cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "      \n",
    "        if  pivot_col!='weekend' and  pivot_col!='installments':\n",
    "            tmp            = df.groupby(['card_id',pivot_col])['purchase_date_floorday'].nunique().to_frame(pivot_col + '_purchase_date_floorday_nunique') \n",
    "            tmp1           = df.groupby(['card_id'])['purchase_date_floorday'].nunique().to_frame('purchase_date_floorday_nunique') \n",
    "            tmp.reset_index(inplace =True)  \n",
    "            tmp1.reset_index(inplace =True)   \n",
    "            tmp  = tmp.merge(tmp1, on ='card_id', how='left')\n",
    "            tmp[pivot_col + '_day_nunique_pct'] = tmp[pivot_col + '_purchase_date_floorday_nunique'].values / tmp['purchase_date_floorday_nunique'].values\n",
    "         \n",
    "            tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_day_nunique_pct',fill_value=0)\n",
    "            tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_day_nunique_pct_'+ str(col) for col in tmp_pivot.columns]\n",
    "            tmp_pivot.reset_index(inplace = True)\n",
    "            cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "    \n",
    "    if prefix == 'new_cardf_':\n",
    "    ######## 在卡未激活之前就有过消费的记录  ##############   \n",
    "        print('*'*30,'Part2， data with time less than activation day','*'*30)\n",
    "        df_part = df.loc[df.purchase_date < df.first_active_month]\n",
    "\n",
    "        cardid_features_part = pd.DataFrame()\n",
    "        cardid_features_part['card_id'] = df_part['card_id'].unique()   \n",
    "        ht_card_id_part_gp = df_part.groupby('card_id')\n",
    "        cardid_features_part['card_id_part_cnt'] = ht_card_id_part_gp['authorized_flag'].count().values\n",
    "\n",
    "        print('card_id(purchase_amount): sum') \n",
    "        for col in tqdm_notebook(['purchase_amount']): \n",
    "            for opt in ['sum','mean']:\n",
    "                cardid_features_part['card_id_part_' +col+ '_' + opt] = ht_card_id_part_gp[col].agg(opt).values\n",
    "\n",
    "        cardid_features = cardid_features.merge(cardid_features_part, on ='card_id', how='left')\n",
    "        cardid_features['card_id_part_purchase_amount_sum_percent'] = cardid_features['card_id_part_purchase_amount_sum'] / (cardid_features['card_id_purchase_amount_sum'] + 0.01)\n",
    "\n",
    "    cardid_features = reduce_mem_usage(cardid_features)\n",
    "    \n",
    "    new_col_names = []\n",
    "    for col in cardid_features.columns:\n",
    "        if col == 'card_id':\n",
    "            new_col_names.append(col)\n",
    "        else:\n",
    "            new_col_names.append(prefix + col)\n",
    "    cardid_features.columns = new_col_names\n",
    "    \n",
    "    return cardid_features\n",
    "print('auth...')\n",
    "authorized_transactions = historical_transactions.loc[historical_transactions['authorized_flag'] == 1]\n",
    "auth_cardf_tran = get_cardf_tran(authorized_transactions, 3, prefix='auth_cardf_')\n",
    "print('hist...')\n",
    "hist_cardf_tran = get_cardf_tran(historical_transactions, 3, prefix='hist_cardf_')\n",
    "print('new...')\n",
    "reference_days = historical_transactions.groupby('card_id')['purchase_date'].last().to_frame('reference_day')\n",
    "reference_days.reset_index(inplace = True)\n",
    "new_transactions = new_transactions.merge(reference_days, on ='card_id', how='left')\n",
    "new_cardf_tran  = get_cardf_tran(new_transactions, 5, prefix='new_cardf_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，需要进一步考虑最近两个月的用户行为特征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def get_cardf_tran_last2(df_, month = 3, prefix = 'last2_'): \n",
    "    \n",
    "    df = df_.loc[df_.month_lag >= -2].copy()\n",
    "    print('*'*30,'Part1, whole data','*'*30)\n",
    "    cardid_features = pd.DataFrame()\n",
    "    cardid_features['card_id'] = df['card_id'].unique()   \n",
    "    \n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] = df['month_diff'].astype(int)\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    print( '*' * 30, 'Traditional Features', '*' * 30)\n",
    "    ht_card_id_gp = df.groupby('card_id')\n",
    "    print(' card id : count')\n",
    "    cardid_features['card_id_cnt'] = ht_card_id_gp['authorized_flag'].count().values\n",
    "    \n",
    "    cardid_features['card_id_isau_mean'] = ht_card_id_gp['authorized_flag'].mean().values \n",
    "    cardid_features['card_id_isau_sum']  = ht_card_id_gp['authorized_flag'].sum().values\n",
    "    \n",
    "    cardid_features['month_diff_mean']   = ht_card_id_gp['month_diff'].mean().values\n",
    "\n",
    "    print('card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique') \n",
    "    for col in tqdm_notebook(['state_id','city_id','installments','merchant_id', 'merchant_category_id','purchase_date_floorday']):\n",
    "        cardid_features['card_id_%s_nunique'%col] = ht_card_id_gp[col].nunique().values\n",
    "        cardid_features['card_id_cnt_divide_%s_nunique'%col] = cardid_features['card_id_cnt'].values / cardid_features['card_id_%s_nunique'%col].values\n",
    "         \n",
    "    for col in tqdm_notebook(['purchase_amount','purchase_amount_ddgd_98','purchase_amount_wdgd_96','purchase_amount_mdgd_90','purchase_amount_mdgd_80']): #,'purchase_amount_ddgd_98','purchase_amount_ddgd_99','purchase_amount_wdgd_96','purchase_amount_wdgd_97','purchase_amount_mdgd_90','purchase_amount_mdgd_80']):\n",
    "        if col =='purchase_amount':\n",
    "            for opt in ['sum','mean','std','median']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values  \n",
    "        else:\n",
    "            for opt in ['sum']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values \n",
    "    \n",
    "    print( '*' * 30, 'Pivot Features', '*' * 30)\n",
    "    print('Count  Pivot') #purchase_month_since_reference_day(可能和month_lag重复),百分比降分,暂时忽略 (dayofweek,merchant_cate,state_id)作用不大\n",
    "    \n",
    "    for pivot_col in tqdm_notebook(['category_1','category_2','category_3','month_lag','subsector_id','weekend']): #'city_id', \n",
    "    \n",
    "        tmp     = df.groupby(['card_id',pivot_col])['merchant_id'].count().to_frame(pivot_col + '_count')\n",
    "        tmp.reset_index(inplace =True)  \n",
    "         \n",
    "        tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_count',fill_value=0)\n",
    "        tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_cnt_pivot_'+ str(col) for col in tmp_pivot.columns]\n",
    "        tmp_pivot.reset_index(inplace = True)\n",
    "        cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "      \n",
    "        if  pivot_col!='weekend' and  pivot_col!='installments':\n",
    "            tmp            = df.groupby(['card_id',pivot_col])['purchase_date_floorday'].nunique().to_frame(pivot_col + '_purchase_date_floorday_nunique') \n",
    "            tmp1           = df.groupby(['card_id'])['purchase_date_floorday'].nunique().to_frame('purchase_date_floorday_nunique') \n",
    "            tmp.reset_index(inplace =True)  \n",
    "            tmp1.reset_index(inplace =True)   \n",
    "            tmp  = tmp.merge(tmp1, on ='card_id', how='left')\n",
    "            tmp[pivot_col + '_day_nunique_pct'] = tmp[pivot_col + '_purchase_date_floorday_nunique'].values / tmp['purchase_date_floorday_nunique'].values\n",
    "         \n",
    "            tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_day_nunique_pct',fill_value=0)\n",
    "            tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_day_nunique_pct_'+ str(col) for col in tmp_pivot.columns]\n",
    "            tmp_pivot.reset_index(inplace = True)\n",
    "            cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "     \n",
    "    cardid_features = reduce_mem_usage(cardid_features)\n",
    "    \n",
    "    new_col_names = []\n",
    "    for col in cardid_features.columns:\n",
    "        if col == 'card_id':\n",
    "            new_col_names.append(col)\n",
    "        else:\n",
    "            new_col_names.append(prefix + col)\n",
    "    cardid_features.columns = new_col_names\n",
    "    \n",
    "    return cardid_features  \n",
    "\n",
    "hist_cardf_tran_last2 = get_cardf_tran_last2(historical_transactions, month = 3, prefix = 'hist_last2_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以及进一步进行二阶交叉特征衍生："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def successive_aggregates(df_, prefix = 'levelAB_'):\n",
    "    df = df_.copy()\n",
    "    cardid_features = pd.DataFrame()\n",
    "    cardid_features['card_id'] = df['card_id'].unique()    \n",
    "     \n",
    "    level12_nunique = [('month_lag','state_id'),('month_lag','city_id'),('month_lag','subsector_id'),('month_lag','merchant_category_id'),('month_lag','merchant_id'),('month_lag','purchase_date_floorday'),\\\n",
    "                       ('subsector_id','merchant_category_id'),('subsector_id','merchant_id'),('subsector_id','purchase_date_floorday'),('subsector_id','month_lag'),\\\n",
    "                       ('merchant_category_id', 'merchant_id'),('merchant_category_id','purchase_date_floorday'),('merchant_category_id','month_lag'),\\\n",
    "                       ('purchase_date_floorday', 'merchant_id'),('purchase_date_floorday','merchant_category_id'),('purchase_date_floorday','subsector_id')]    \n",
    "    for col_level1,col_level2 in tqdm_notebook(level12_nunique):  \n",
    "        \n",
    "        level1  = df.groupby(['card_id',col_level1])[col_level2].nunique().to_frame(col_level2 + '_nunique')\n",
    "        level1.reset_index(inplace =True)  \n",
    "         \n",
    "        level2 = level1.groupby('card_id')[col_level2 + '_nunique'].agg(['mean', 'max', 'std'])\n",
    "        level2 = pd.DataFrame(level2)\n",
    "        level2.columns = [col_level1 + '_' + col_level2 + '_nunique_' + col for col in level2.columns.values]\n",
    "        level2.reset_index(inplace = True)\n",
    "        \n",
    "        cardid_features = cardid_features.merge(level2, on='card_id', how='left') \n",
    "    \n",
    "    level12_count = ['month_lag','state_id','city_id','subsector_id','merchant_category_id','merchant_id','purchase_date_floorday']\n",
    "    for col_level in tqdm_notebook(level12_count): \n",
    "    \n",
    "        level1  = df.groupby(['card_id',col_level])['merchant_id'].count().to_frame(col_level + '_count')\n",
    "        level1.reset_index(inplace =True)  \n",
    "         \n",
    "        level2 = level1.groupby('card_id')[col_level + '_count'].agg(['mean', 'max', 'std'])\n",
    "        level2 = pd.DataFrame(level2)\n",
    "        level2.columns = [col_level + '_count_' + col for col in level2.columns.values]\n",
    "        level2.reset_index(inplace = True)\n",
    "        \n",
    "        cardid_features = cardid_features.merge(level2, on='card_id', how='left') \n",
    "    \n",
    "    level12_meansum = [('month_lag','purchase_amount'),('state_id','purchase_amount'),('city_id','purchase_amount'),('subsector_id','purchase_amount'),\\\n",
    "                       ('merchant_category_id','purchase_amount'),('merchant_id','purchase_amount'),('purchase_date_floorday','purchase_amount')]\n",
    "    for col_level1,col_level2 in tqdm_notebook(level12_meansum): \n",
    "    \n",
    "        level1  = df.groupby(['card_id',col_level1])[col_level2].sum().to_frame(col_level2 + '_sum')\n",
    "        level1.reset_index(inplace =True)  \n",
    "         \n",
    "        level2 = level1.groupby('card_id')[col_level2 + '_sum'].agg(['mean', 'max', 'std'])\n",
    "        level2 = pd.DataFrame(level2)\n",
    "        level2.columns = [col_level1 + '_' + col_level2 + '_sum_' + col for col in level2.columns.values]\n",
    "        level2.reset_index(inplace = True)\n",
    "\n",
    "        cardid_features = cardid_features.merge(level2, on='card_id', how='left')           \n",
    "    \n",
    "    cardid_features = reduce_mem_usage(cardid_features)\n",
    "    \n",
    "    new_col_names = []\n",
    "    for col in cardid_features.columns:\n",
    "        if col == 'card_id':\n",
    "            new_col_names.append(col)\n",
    "        else:\n",
    "            new_col_names.append(prefix + col)\n",
    "    cardid_features.columns = new_col_names\n",
    "    \n",
    "    return cardid_features  \n",
    "\n",
    "print('hist...')\n",
    "hist_levelAB = successive_aggregates(historical_transactions, prefix = 'hist_levelAB_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，将上述衍生特征进行合并："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "## 合并到训练集和测试集\n",
    "print('#_____基础统计特征')\n",
    "train = pd.merge(train, auth_base_stat, on='card_id', how='left')\n",
    "test  = pd.merge(test,  auth_base_stat, on='card_id', how='left')\n",
    "train = pd.merge(train, hist_base_stat, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_base_stat, on='card_id', how='left')\n",
    "train = pd.merge(train, new_base_stat , on='card_id', how='left')\n",
    "test  = pd.merge(test,  new_base_stat , on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print('#_____全局cardid特征')\n",
    "train = pd.merge(train, auth_cardf_tran, on='card_id', how='left')\n",
    "test  = pd.merge(test,  auth_cardf_tran, on='card_id', how='left')\n",
    "train = pd.merge(train, hist_cardf_tran, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_cardf_tran, on='card_id', how='left')\n",
    "train = pd.merge(train, new_cardf_tran , on='card_id', how='left')\n",
    "test  = pd.merge(test,  new_cardf_tran , on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print('#_____最近两月cardid特征')\n",
    "train = pd.merge(train, hist_cardf_tran_last2, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_cardf_tran_last2, on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print('#_____补充二阶特征')\n",
    "train = pd.merge(train, hist_levelAB, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_levelAB, on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "并在此基础上补充部分简单四则运算后的衍生特征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train['outliers'] = 0\n",
    "train.loc[train['target'] < -30, 'outliers'] = 1\n",
    "train['outliers'].value_counts()\n",
    "for f in ['feature_1','feature_2','feature_3']:\n",
    "    colname = f+'_outliers_mean'\n",
    "    order_label = train.groupby([f])['outliers'].mean()\n",
    "    for df in [train, test]:\n",
    "        df[colname] = df[f].map(order_label)\n",
    "\n",
    "for df in [train, test]:\n",
    "    \n",
    "    df['days_feature1'] = df['elapsed_time'] * df['feature_1']\n",
    "    df['days_feature2'] = df['elapsed_time'] * df['feature_2']\n",
    "    df['days_feature3'] = df['elapsed_time'] * df['feature_3']\n",
    "\n",
    "    df['days_feature1_ratio'] = df['feature_1'] / df['elapsed_time']\n",
    "    df['days_feature2_ratio'] = df['feature_2'] / df['elapsed_time']\n",
    "    df['days_feature3_ratio'] = df['feature_3'] / df['elapsed_time']\n",
    "\n",
    "    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\n",
    "    df['feature_mean'] = df['feature_sum']/3\n",
    "    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n",
    "    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n",
    "    df['feature_var'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n",
    "    \n",
    "    df['card_id_total'] = df['hist_card_id_size']+df['new_card_id_size']\n",
    "    df['card_id_cnt_total'] = df['hist_card_id_count']+df['new_card_id_count']\n",
    "    df['card_id_cnt_ratio'] = df['new_card_id_count']/df['hist_card_id_count']\n",
    "    df['purchase_amount_total'] = df['hist_cardf_card_id_purchase_amount_sum']+df['new_cardf_card_id_purchase_amount_sum']\n",
    "    df['purchase_amount_ratio'] = df['new_cardf_card_id_purchase_amount_sum']/df['hist_cardf_card_id_purchase_amount_sum']\n",
    "    df['month_diff_ratio'] = df['new_cardf_month_diff_mean']/df['hist_cardf_month_diff_mean']\n",
    "    df['installments_total'] = df['new_cardf_card_id_installments_sum']+df['auth_cardf_card_id_installments_sum']\n",
    "    df['installments_ratio'] = df['new_cardf_card_id_installments_sum']/df['auth_cardf_card_id_installments_sum']\n",
    "    df['price_total'] = df['purchase_amount_total']/df['installments_total']\n",
    "    df['new_CLV'] = df['new_card_id_count'] * df['new_cardf_card_id_purchase_amount_sum'] / df['new_cardf_month_diff_mean']\n",
    "    df['hist_CLV'] = df['hist_card_id_count'] * df['hist_cardf_card_id_purchase_amount_sum'] / df['hist_cardf_month_diff_mean']\n",
    "    df['CLV_ratio'] = df['new_CLV'] / df['hist_CLV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.特征筛选\n",
    "在创建完全部特征后即可进行特征筛选了。此处我们考虑手动进行特征筛选，排除部分过于稀疏的特征后即可将数据保存在本地："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "del_cols = []\n",
    "for col in train.columns:\n",
    "    if 'subsector_id_cnt_' in col and 'new_cardf' in col: \n",
    "        del_cols.append(col)\n",
    "del_cols1 = []\n",
    "for col in train.columns:\n",
    "    if 'subsector_id_cnt_' in col and 'hist_last2_' in col:\n",
    "        del_cols1.append(col)\n",
    "del_cols2 = []\n",
    "for col in train.columns:\n",
    "    if 'subsector_id_cnt_' in col and 'auth_cardf' in col:\n",
    "        del_cols2.append(col)\n",
    "del_cols3 = []\n",
    "for col in train.columns:\n",
    "    if 'merchant_category_id_month_lag_nunique_' in col and '_pivot_supp' in col:\n",
    "        del_cols3.append(col)\n",
    "    if 'city_id' in col and '_pivot_supp' in col:\n",
    "        del_cols3.append(col)\n",
    "    if 'month_diff' in col and 'hist_last2_' in col:\n",
    "        del_cols3.append(col)\n",
    "    if 'month_diff_std' in col or 'month_diff_gap' in col:\n",
    "        del_cols3.append(col) \n",
    "fea_cols = [col for col in train.columns if train[col].dtypes!='object' and train[col].dtypes != '<M8[ns]' and col!='target' not in col and col!='min_num'\\\n",
    "            and col not in del_cols and col not in del_cols1 and col not in del_cols2 and col!='target1' and col!='card_id_cnt_ht_pivot_supp'  and col not in del_cols3]   \n",
    "print('删除前:',train.shape[1])\n",
    "print('删除后:',len(fea_cols))\n",
    "\n",
    "train = train[fea_cols+['target']]\n",
    "fea_cols.remove('outliers')\n",
    "test = test[fea_cols]\n",
    "\n",
    "train.to_csv('./data/all_train_features.csv',index=False)\n",
    "test.to_csv('./data/all_test_features.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际执行过程中，可以按照如下方式进行读取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## load all features\n",
    "train = pd.read_csv('./data/all_train_features.csv')\n",
    "test  = pd.read_csv('./data/all_test_features.csv')\n",
    "\n",
    "inf_cols = ['new_cardf_card_id_cnt_divide_installments_nunique', 'hist_last2_card_id_cnt_divide_installments_nunique']\n",
    "train[inf_cols] = train[inf_cols].replace(np.inf, train[inf_cols].replace(np.inf, -99).max().max())\n",
    "ntrain[inf_cols] = ntrain[inf_cols].replace(np.inf, ntrain[inf_cols].replace(np.inf, -99).max().max())\n",
    "test[inf_cols] = test[inf_cols].replace(np.inf, test[inf_cols].replace(np.inf, -99).max().max())\n",
    "\n",
    "# ## load sparse\n",
    "# train_tags = sparse.load_npz('train_tags.npz')\n",
    "# test_tags  = sparse.load_npz('test_tags.npz')\n",
    "\n",
    "## 获取非异常值的index\n",
    "normal_index = train[train['outliers']==0].index.tolist()\n",
    "## without outliers\n",
    "ntrain = train[train['outliers'] == 0]\n",
    "\n",
    "target        = train['target'].values\n",
    "ntarget       = ntrain['target'].values\n",
    "target_binary = train['outliers'].values\n",
    "###\n",
    "y_train        = target\n",
    "y_ntrain       = ntarget\n",
    "y_train_binary = target_binary\n",
    "\n",
    "print('train:',train.shape)\n",
    "print('ntrain:',ntrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>二、两阶段建模优化\n",
    "## <center> LightGBM+XGBoost+CatBoost两阶段建模\n",
    "\n",
    "<center><img src=\"https://s2.loli.net/2021/12/10/a19mJpQOkMunDbf.png\" alt=\"image-20211210165529875\" style=\"zoom:33%;\" />\n",
    "\n",
    "为了方便更快速的调用三种不同的模型，并且同时要求能够完成分类和回归预测，此处通过定义一个函数来完成所有模型的训练过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, params, folds, model_type='lgb', eval_type='regression'):\n",
    "    oof = np.zeros(X.shape[0])\n",
    "    predictions = np.zeros(X_test.shape[0])\n",
    "    scores = []\n",
    "    for fold_n, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            trn_data = lgb.Dataset(X[trn_idx], y[trn_idx])\n",
    "            val_data = lgb.Dataset(X[val_idx], y[val_idx])\n",
    "            clf = lgb.train(params, trn_data, num_boost_round=20000, \n",
    "                            valid_sets=[trn_data, val_data], \n",
    "                            verbose_eval=100, early_stopping_rounds=300)\n",
    "            oof[val_idx] = clf.predict(X[val_idx], num_iteration=clf.best_iteration)\n",
    "            predictions += clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "        \n",
    "        if model_type == 'xgb':\n",
    "            trn_data = xgb.DMatrix(X[trn_idx], y[trn_idx])\n",
    "            val_data = xgb.DMatrix(X[val_idx], y[val_idx])\n",
    "            watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "            clf = xgb.train(dtrain=trn_data, num_boost_round=20000, \n",
    "                            evals=watchlist, early_stopping_rounds=200, \n",
    "                            verbose_eval=100, params=params)\n",
    "            oof[val_idx] = clf.predict(xgb.DMatrix(X[val_idx]), ntree_limit=clf.best_ntree_limit)\n",
    "            predictions += clf.predict(xgb.DMatrix(X_test), ntree_limit=clf.best_ntree_limit) / folds.n_splits\n",
    "        \n",
    "        if (model_type == 'cat') and (eval_type == 'regression'):\n",
    "            clf = CatBoostRegressor(iterations=20000, eval_metric='RMSE', **params)\n",
    "            clf.fit(X[trn_idx], y[trn_idx], \n",
    "                    eval_set=(X[val_idx], y[val_idx]),\n",
    "                    cat_features=[], use_best_model=True, verbose=100)\n",
    "            oof[val_idx] = clf.predict(X[val_idx])\n",
    "            predictions += clf.predict(X_test) / folds.n_splits\n",
    "            \n",
    "        if (model_type == 'cat') and (eval_type == 'binary'):\n",
    "            clf = CatBoostClassifier(iterations=20000, eval_metric='Logloss', **params)\n",
    "            clf.fit(X[trn_idx], y[trn_idx], \n",
    "                    eval_set=(X[val_idx], y[val_idx]),\n",
    "                    cat_features=[], use_best_model=True, verbose=100)\n",
    "            oof[val_idx] = clf.predict_proba(X[val_idx])[:,1]\n",
    "            predictions += clf.predict_proba(X_test)[:,1] / folds.n_splits\n",
    "        print(predictions)\n",
    "        if eval_type == 'regression':\n",
    "            scores.append(mean_squared_error(oof[val_idx], y[val_idx])**0.5)\n",
    "        if eval_type == 'binary':\n",
    "            scores.append(log_loss(y[val_idx], oof[val_idx]))\n",
    "        \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    return oof, predictions, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- LightGBM模型训练\n",
    "\n",
    "接下来即可进行LGB模型训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### lgb\n",
    "lgb_params = {'num_leaves': 63,\n",
    "             'min_data_in_leaf': 32, \n",
    "             'objective':'regression',\n",
    "             'max_depth': -1,\n",
    "             'learning_rate': 0.01,\n",
    "             \"min_child_samples\": 20,\n",
    "             \"boosting\": \"gbdt\",\n",
    "             \"feature_fraction\": 0.9,\n",
    "             \"bagging_freq\": 1,\n",
    "             \"bagging_fraction\": 0.9 ,\n",
    "             \"bagging_seed\": 11,\n",
    "             \"metric\": 'rmse',\n",
    "             \"lambda_l1\": 0.1,\n",
    "             \"verbosity\": -1}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=4096)\n",
    "X_ntrain = ntrain[fea_cols].values\n",
    "X_train  = train[fea_cols].values\n",
    "X_test   = test[fea_cols].values\n",
    "print('='*10,'回归模型','='*10)\n",
    "oof_lgb , predictions_lgb , scores_lgb  = train_model(X_train , X_test, y_train, params=lgb_params, folds=folds, model_type='lgb', eval_type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*10,'without outliers 回归模型','='*10)\n",
    "oof_nlgb, predictions_nlgb, scores_nlgb = train_model(X_ntrain, X_test, y_ntrain, params=lgb_params, folds=folds, model_type='lgb', eval_type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*10,'分类模型','='*10)\n",
    "lgb_params['objective'] = 'binary'\n",
    "lgb_params['metric']    = 'binary_logloss'\n",
    "oof_blgb, predictions_blgb, scores_blgb = train_model(X_train , X_test, y_train_binary, params=lgb_params, folds=folds, model_type='lgb', eval_type='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后将所有预测结果进行保存，包括一个分类模型、以及两个回归模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_lgb\n",
    "sub_df.to_csv('predictions_lgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_lgb  = pd.DataFrame(oof_lgb)\n",
    "oof_nlgb = pd.DataFrame(oof_nlgb)\n",
    "oof_blgb = pd.DataFrame(oof_blgb)\n",
    "\n",
    "predictions_lgb  = pd.DataFrame(predictions_lgb)\n",
    "predictions_nlgb = pd.DataFrame(predictions_nlgb)\n",
    "predictions_blgb = pd.DataFrame(predictions_blgb)\n",
    "\n",
    "oof_lgb.to_csv('./result/oof_lgb.csv',header=None,index=False)\n",
    "oof_blgb.to_csv('./result/oof_blgb.csv',header=None,index=False)\n",
    "oof_nlgb.to_csv('./result/oof_nlgb.csv',header=None,index=False)\n",
    "\n",
    "predictions_lgb.to_csv('./result/predictions_lgb.csv',header=None,index=False)\n",
    "predictions_nlgb.to_csv('./result/predictions_nlgb.csv',header=None,index=False)\n",
    "predictions_blgb.to_csv('./result/predictions_blgb.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- xgboost模型训练\n",
    "\n",
    "接下来进一步进行XGB模型训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### xgb\n",
    "xgb_params = {'eta':0.05, 'max_leaves':47, 'max_depth':10, 'subsample':0.8, 'colsample_bytree':0.8,\n",
    "              'min_child_weight':40, 'max_bin':128, 'reg_alpha':2.0, 'reg_lambda':2.0, \n",
    "              'objective':'reg:linear', 'eval_metric':'rmse', 'silent': True, 'nthread':4}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "print('='*10,'回归模型','='*10)\n",
    "oof_xgb , predictions_xgb , scores_xgb  = train_model(X_train , X_test, y_train , params=xgb_params, folds=folds, model_type='xgb', eval_type='regression')\n",
    "print('='*10,'without outliers 回归模型','='*10)\n",
    "oof_nxgb, predictions_nxgb, scores_nxgb = train_model(X_ntrain, X_test, y_ntrain, params=xgb_params, folds=folds, model_type='xgb', eval_type='regression')\n",
    "print('='*10,'分类模型','='*10)\n",
    "xgb_params['objective'] = 'binary:logistic'\n",
    "xgb_params['metric']    = 'binary_logloss'\n",
    "oof_bxgb, predictions_bxgb, scores_bxgb = train_model(X_train , X_test, y_train_binary, params=xgb_params, folds=folds, model_type='xgb', eval_type='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后进行结果保存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_xgb\n",
    "sub_df.to_csv('predictions_xgb.csv', index=False)\n",
    "\n",
    "oof_xgb  = pd.DataFrame(oof_xgb)\n",
    "oof_nxgb = pd.DataFrame(oof_nxgb)\n",
    "oof_bxgb = pd.DataFrame(oof_bxgb)\n",
    "\n",
    "predictions_xgb  = pd.DataFrame(predictions_xgb)\n",
    "predictions_nxgb = pd.DataFrame(predictions_nxgb)\n",
    "predictions_bxgb = pd.DataFrame(predictions_bxgb)\n",
    "\n",
    "oof_xgb.to_csv('./result/oof_xgb.csv',header=None,index=False)\n",
    "oof_bxgb.to_csv('./result/oof_bxgb.csv',header=None,index=False)\n",
    "oof_nxgb.to_csv('./result/oof_nxgb.csv',header=None,index=False)\n",
    "\n",
    "predictions_xgb.to_csv('./result/predictions_xgb.csv',header=None,index=False)\n",
    "predictions_nxgb.to_csv('./result/predictions_nxgb.csv',header=None,index=False)\n",
    "predictions_bxgb.to_csv('./result/predictions_bxgb.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CatBoost\n",
    "\n",
    "接下来进行CatBoost模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### cat\n",
    "cat_params = {'learning_rate': 0.05, 'depth': 9, 'l2_leaf_reg': 10, 'bootstrap_type': 'Bernoulli',\n",
    "              'od_type': 'Iter', 'od_wait': 50, 'random_seed': 11, 'allow_writing_files': False}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=18)\n",
    "print('='*10,'回归模型','='*10)\n",
    "oof_cat , predictions_cat , scores_cat  = train_model(X_train , X_test, y_train , params=cat_params, folds=folds, model_type='cat', eval_type='regression')\n",
    "print('='*10,'without outliers 回归模型','='*10)\n",
    "oof_ncat, predictions_ncat, scores_ncat = train_model(X_ntrain, X_test, y_ntrain, params=cat_params, folds=folds, model_type='cat', eval_type='regression')\n",
    "print('='*10,'分类模型','='*10)\n",
    "oof_bcat, predictions_bcat, scores_bcat = train_model(X_train , X_test, y_train_binary, params=cat_params, folds=folds, model_type='cat', eval_type='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时保存模型结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_cat\n",
    "sub_df.to_csv('predictions_cat.csv', index=False)\n",
    "\n",
    "oof_cat  = pd.DataFrame(oof_cat)\n",
    "oof_ncat = pd.DataFrame(oof_ncat)\n",
    "oof_bcat = pd.DataFrame(oof_bcat)\n",
    "\n",
    "predictions_cat  = pd.DataFrame(predictions_cat)\n",
    "predictions_ncat = pd.DataFrame(predictions_ncat)\n",
    "predictions_bcat = pd.DataFrame(predictions_bcat)\n",
    "\n",
    "oof_cat.to_csv('./result/oof_cat.csv',header=None,index=False)\n",
    "oof_bcat.to_csv('./result/oof_bcat.csv',header=None,index=False)\n",
    "oof_ncat.to_csv('./result/oof_ncat.csv',header=None,index=False)\n",
    "\n",
    "predictions_cat.to_csv('./result/predictions_cat.csv',header=None,index=False)\n",
    "predictions_ncat.to_csv('./result/predictions_ncat.csv',header=None,index=False)\n",
    "predictions_bcat.to_csv('./result/predictions_bcat.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.融合阶段\n",
    "\n",
    "- 加权融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = (predictions_lgb + predictions_xgb.values.flatten() + predictions_cat.values.flatten()) / 3\n",
    "sub_df.to_csv('predictions_wei_average.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stacking融合\n",
    "\n",
    "<center><img src=\"https://s2.loli.net/2021/12/08/ALF3cfuSwmB7b8z.png\" alt=\"image-20211208192640281\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_model(oof_1, oof_2, oof_3, predictions_1, predictions_2, predictions_3, y, eval_type='regression'):\n",
    "   \n",
    "    # Part 1.数据准备\n",
    "    # 按行拼接列，拼接验证集所有预测结果\n",
    "    # train_stack就是final model的训练数据\n",
    "    train_stack = np.hstack([oof_1, oof_2, oof_3])\n",
    "    # 按行拼接列，拼接测试集上所有预测结果\n",
    "    # test_stack就是final model的测试数据\n",
    "    test_stack = np.hstack([predictions_1, predictions_2, predictions_3])\n",
    "    # 创建一个和验证集行数相同的全零数组\n",
    "    oof = np.zeros(train_stack.shape[0])\n",
    "    # 创建一个和测试集行数相同的全零数组\n",
    "    predictions = np.zeros(test_stack.shape[0])\n",
    "    \n",
    "    # Part 2.多轮交叉验证\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=2020)\n",
    "    \n",
    "    # fold_为折数，trn_idx为每一折训练集index，val_idx为每一折验证集index\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, y)):\n",
    "        # 打印折数信息\n",
    "        print(\"fold n°{}\".format(fold_+1))\n",
    "        # 训练集中划分为训练数据的特征和标签\n",
    "        trn_data, trn_y = train_stack[trn_idx], y[trn_idx]\n",
    "        # 训练集中划分为验证数据的特征和标签\n",
    "        val_data, val_y = train_stack[val_idx], y[val_idx]\n",
    "        # 开始训练时提示\n",
    "        print(\"-\" * 10 + \"Stacking \" + str(fold_+1) + \"-\" * 10)\n",
    "        # 采用贝叶斯回归作为结果融合的模型（final model）\n",
    "        clf = BayesianRidge()\n",
    "        # 在训练数据上进行训练\n",
    "        clf.fit(trn_data, trn_y)\n",
    "        # 在验证数据上进行预测，并将结果记录在oof对应位置\n",
    "        # oof[val_idx] = clf.predict(val_data)\n",
    "        # 对测试集数据进行预测，每一轮预测结果占比额外的1/10\n",
    "        predictions += clf.predict(test_stack) / (5 * 2)\n",
    "        \n",
    "    if eval_type == 'regression':\n",
    "        print('mean: ',np.sqrt(mean_squared_error(y, oof)))\n",
    "    if eval_type == 'binary':\n",
    "        print('mean: ',log_loss(y, oof))\n",
    "    \n",
    "    # 返回测试集的预测结果\n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*30)\n",
    "oof_stack , predictions_stack  = stack_model(oof_lgb , oof_xgb , oof_cat , predictions_lgb , predictions_xgb , predictions_cat , target)\n",
    "print('='*30)\n",
    "oof_nstack, predictions_nstack = stack_model(oof_nlgb, oof_nxgb, oof_ncat, predictions_nlgb, predictions_nxgb, predictions_ncat, ntarget)\n",
    "print('='*30)\n",
    "oof_bstack, predictions_bstack = stack_model(oof_blgb, oof_bxgb, oof_bcat, predictions_blgb, predictions_bxgb, predictions_bcat, target_binary, eval_type='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_stack\n",
    "sub_df.to_csv('predictions_stack.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trick融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_bstack*-33.219281 + (1-predictions_bstack)*predictions_nstack\n",
    "sub_df.to_csv('predictions_trick.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TrickStacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = (predictions_bstack*-33.219281 + (1-predictions_bstack)*predictions_nstack)*0.5 + predictions_stack*0.5\n",
    "sub_df.to_csv('predictions_trick&stacking.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
