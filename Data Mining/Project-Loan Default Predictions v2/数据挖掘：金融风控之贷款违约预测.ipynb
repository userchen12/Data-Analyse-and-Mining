{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cd9c2a-d9e5-4b25-a1ed-e5f03ba2d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt    \n",
    "import seaborn as sns    \n",
    "import datetime    \n",
    "import warnings    \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426eab3b-2388-4cf4-9d15-e21fc5b59199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据\n",
    "import pandas as pd\n",
    "data_train = pd.read_csv('train.csv')    \n",
    "data_test_a = pd.read_csv('testA.csv')\n",
    "print('Train data shape:',data_train.shape)    \n",
    "print('TestA data shape:',data_test_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a333e229-1e59-4ec7-ae77-366f44ea4708",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54caa80-3790-4fa9-a701-8f45a5539c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac503cf-7ccb-4ca4-9443-434ea3b74c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head(3).append(data_train.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c8bb53-2ab1-40c5-a617-adab2fd73bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {data_train.isnull().any().sum()} columns in train dataset with missing values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865ef93-1dcb-4a67-8c23-ed11681aacc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#进一步查看缺失特征中缺失率大于50%的特征\n",
    "have_null_fea_dict = (data_train.isnull().sum()/len(data_train)).to_dict()    \n",
    "fea_null_moreThanHalf = {}    \n",
    "for key,value in have_null_fea_dict.items():    \n",
    "    if value > 0.5:    \n",
    "        fea_null_moreThanHalf[key] = value\n",
    "fea_null_moreThanHalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84bf612-2959-447e-b66d-08342975bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#缺失率可视化    \n",
    "missing = data_train.isnull().sum()/len(data_train)    \n",
    "missing = missing[missing > 0]    \n",
    "missing.sort_values(inplace=True)    \n",
    "missing.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0336480-383b-4a81-928b-e510088532e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看一值特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4751cf0-84fd-4a00-bc3e-0a463d7b5d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_value_fea = [col for col in data_train.columns if data_train[col].nunique() <= 1]\n",
    "one_value_fea_test = [col for col in data_test_a.columns if data_test_a[col].nunique() <= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7f1578-5708-4bc5-b719-d0945d787e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_value_fea, one_value_fea_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b53a9ff-ae1b-4a53-bfee-629106704cfc",
   "metadata": {},
   "source": [
    "## 查看特征的数值类型有哪些，对象类型有哪些\n",
    "- 特征一般都是由类别型特征和数值型特征组成，而数值型特征又分为连续型和离散型。\n",
    "- 类别型特征有时具有非数值关系，有时也具有数值关系。比如‘grade’中的等级A，B，C等，是否只是单纯的分类，还是A优于其他要结合业务判断。\n",
    "- 数值型特征本是可以直接入模的，但往往风控人员要对其做分箱，转化为WOE编码进而做标准评分卡等操作。从模型效果上来看，特征分箱主要是为了降低变量的复杂性，减少变量噪音对模型的影响，提高自变量和因变量的相关度。从而使模型更加稳定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c3db7-1348-44d6-b0ff-7032d34fbbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_fea = list(data_train.select_dtypes(exclude=['object']).columns)  \n",
    "category_fea = list(filter(lambda x: x not in numerical_fea,list(data_train.columns)))\n",
    "numerical_fea.remove('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7489d5-4bca-462e-91f0-f64ac7f214f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分数值型变量中的连续变量和离散型变量\n",
    "#过滤数值型类别特征    \n",
    "def get_numerical_serial_fea(data,feas, threshold=10):    \n",
    "    numerical_serial_fea = []    \n",
    "    numerical_noserial_fea = []    \n",
    "    for fea in feas:    \n",
    "        temp = data[fea].nunique()    \n",
    "        if temp <= threshold:    \n",
    "            numerical_noserial_fea.append(fea)    \n",
    "            continue    \n",
    "        numerical_serial_fea.append(fea)    \n",
    "    return numerical_serial_fea,numerical_noserial_fea    \n",
    "numerical_serial_fea,numerical_noserial_fea = get_numerical_serial_fea(data_train,numerical_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0fc7c8-8f45-431b-991f-aa584c56f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看每个离散数值变量的分布\n",
    "for value in numerical_noserial_fea:\n",
    "    print(value, ':')\n",
    "    print(data_train[value].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7709c48d-7c7a-4d8a-be7d-b83f33a62844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#可以看到policyCode无用，n11和n12分布差距悬殊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4445b5cd-d404-47d4-8b5b-52526773d565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#查看数值性连续变量的分布\n",
    "#pd.melt将多列转化成variable+value的两列形式\n",
    "f = pd.melt(data_train, value_vars=numerical_serial_fea)    \n",
    "#sns.FacetGrid根据variable进行分类\n",
    "g = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False)    \n",
    "#g.map，将value设为横坐标\n",
    "g = g.map(sns.distplot, \"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff91e32-cab9-45d2-bae6-b2e55223d9d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#查看类别变量的分布\n",
    "# category_fea.remove('id')\n",
    "for var in category_fea:\n",
    "    print(var,':')\n",
    "    print(data_train[var].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e93e11-ff22-406d-80b3-50d08fc1f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#首先查看类别型变量在不同y值上的分布\n",
    "train_loan_fr = data_train.loc[data_train['isDefault'] == 1]    \n",
    "train_loan_nofr = data_train.loc[data_train['isDefault'] == 0]\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 8))    \n",
    "train_loan_fr.groupby('grade')['grade'].count().plot(kind='barh', ax=ax1, title='Count of grade fraud')    \n",
    "train_loan_nofr.groupby('grade')['grade'].count().plot(kind='barh', ax=ax2, title='Count of grade non-fraud')    \n",
    "train_loan_fr.groupby('employmentLength')['employmentLength'].count().plot(kind='barh', ax=ax3, title='Count of employmentLength fraud')    \n",
    "train_loan_nofr.groupby('employmentLength')['employmentLength'].count().plot(kind='barh', ax=ax4, title='Count of employmentLength non-fraud')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a0fb59-adda-4ce3-93aa-85c85fd9b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#其次查看连续型变量在不同y值上的分布\n",
    "fig,((ax1, ax2)) = plt.subplots(1, 2, figsize=(15, 6))    \n",
    "(data_train.loc[data_train['isDefault'] == 1]['loanAmnt'].apply(np.log)   \n",
    "    .plot(kind='hist',    \n",
    "          bins=100,    \n",
    "          title='Log Loan Amt - Fraud',    \n",
    "          color='r',    \n",
    "          xlim=(-3, 10),    \n",
    "         ax= ax1) \n",
    ")\n",
    "(data_train.loc[data_train['isDefault'] == 0]['loanAmnt'].apply(np.log)  \n",
    "    .plot(kind='hist',    \n",
    "          bins=100,    \n",
    "          title='Log Loan Amt - Not Fraud',    \n",
    "          color='b',    \n",
    "          xlim=(-3, 10),    \n",
    "         ax=ax2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7022f39f-f03a-417d-bf80-f245faa45ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(data_train)    \n",
    "total_amt = data_train.groupby(['isDefault'])['loanAmnt'].sum().sum()    \n",
    "plt.figure(figsize=(12,5))    \n",
    "plt.subplot(121)##1代表行，2代表列，所以一共有2个图，1代表此时绘制第一个图。    \n",
    "plot_tr = sns.countplot(x='isDefault',data=data_train)#data_train‘isDefault’这个特征每种类别的数量**    \n",
    "plot_tr.set_title(\"Fraud Loan Distribution \\n 0: good user | 1: bad user\", fontsize=14)    \n",
    "plot_tr.set_xlabel(\"Is fraud by count\", fontsize=16)    \n",
    "plot_tr.set_ylabel('Count', fontsize=16)    \n",
    "for p in plot_tr.patches:    \n",
    "    height = p.get_height()    \n",
    "    plot_tr.text(p.get_x()+p.get_width()/2.,    \n",
    "            height + 3,    \n",
    "            '{:1.2f}%'.format(height/total*100),    \n",
    "            ha=\"center\", fontsize=15)    \n",
    "\n",
    "percent_amt = (data_train.groupby(['isDefault'])['loanAmnt'].sum())    \n",
    "percent_amt = percent_amt.reset_index()    \n",
    "plt.subplot(122)    \n",
    "plot_tr_2 = sns.barplot(x='isDefault', y='loanAmnt',  dodge=True, data=percent_amt)    \n",
    "plot_tr_2.set_title(\"Total Amount in loanAmnt  \\n 0: good user | 1: bad user\", fontsize=14)    \n",
    "plot_tr_2.set_xlabel(\"Is fraud by percent\", fontsize=16)    \n",
    "plot_tr_2.set_ylabel('Total Loan Amount Scalar', fontsize=16)    \n",
    "for p in plot_tr_2.patches:    \n",
    "    height = p.get_height()    \n",
    "    plot_tr_2.text(p.get_x()+p.get_width()/2.,    \n",
    "            height + 3,    \n",
    "            '{:1.2f}%'.format(height/total_amt * 100),    \n",
    "            ha=\"center\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd6f844-70db-4b64-a338-ffaef73ca7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#时间格式数据处理\n",
    "#转化成时间格式  issueDateDT特征表示数据日期离数据集中日期最早的日期（2007-06-01）的天数    \n",
    "data_train['issueDate'] = pd.to_datetime(data_train['issueDate'],format='%Y-%m-%d')    \n",
    "startdate = datetime.datetime.strptime('2007-06-01', '%Y-%m-%d')    \n",
    "data_train['issueDateDT'] = data_train['issueDate'].apply(lambda x: x-startdate).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe5f03c-30ad-4c46-85dd-dfb0c8bb594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#转化成时间格式 test数据   \n",
    "data_test_a['issueDate'] = pd.to_datetime(data_train['issueDate'],format='%Y-%m-%d')    \n",
    "startdate = datetime.datetime.strptime('2007-06-01', '%Y-%m-%d')    \n",
    "data_test_a['issueDateDT'] = data_test_a['issueDate'].apply(lambda x: x-startdate).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a4fee6-298d-4e96-a159-9ab653b1cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_train['issueDateDT'], label='train');    \n",
    "plt.hist(data_test_a['issueDateDT'], label='test');    \n",
    "plt.legend();    \n",
    "plt.title('Distribution of issueDateDT dates');    \n",
    "#train 和 test issueDateDT 日期有重叠 所以使用基于时间的分割进行验证是不明智的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fcf4a8-8d96-48f6-9a7d-464ee15fd766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#透视图 索引可以有多个，“columns（列）”是可选的，聚合函数aggfunc最后是被应用到了变量“values”中你所列举的项目上。    \n",
    "pivot = pd.pivot_table(data_train, index=['grade'], columns=['issueDateDT'], values=['loanAmnt'], aggfunc=np.sum)\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc04b62-06c3-4871-8bdd-79b3f7a90537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#用pandas_profiling生成数据报告\n",
    "# !pip install pandas_profiling\n",
    "import pandas_profiling\n",
    "pfr = pandas_profiling.ProfileReport(data_train)    \n",
    "pfr.to_file(\"./example.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b4fb60-2709-4cb1-a6a8-274aacfa67f5",
   "metadata": {},
   "source": [
    "## 2.4 总结\n",
    "数据探索性分析是我们初步了解数据，熟悉数据为特征工程做准备的阶段，甚至很多时候EDA阶段提取出来的特征可以直接当作规则来用。可见EDA的重要性，这个阶段的主要工作还是借助于各个简单的统计量来对数据整体的了解，分析各个类型变量相互之间的关系，以及用合适的图形可视化出来直观观察。希望本节内容能给初学者带来帮助，更期待各位学习者对其中的不足提出建议。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2913c568-901e-4e03-a5ef-14377a79a967",
   "metadata": {},
   "source": [
    "## 3.特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5a18e-81c9-49d4-96cf-0eebe6c8e907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt    \n",
    "import seaborn as sns    \n",
    "import datetime    \n",
    "from tqdm import tqdm    \n",
    "from sklearn.preprocessing import LabelEncoder    \n",
    "from sklearn.feature_selection import SelectKBest    \n",
    "from sklearn.feature_selection import chi2    \n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "import xgboost as xgb    \n",
    "import lightgbm as lgb    \n",
    "from catboost import CatBoostRegressor    \n",
    "import warnings    \n",
    "from sklearn.model_selection import StratifiedKFold, KFold    \n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss    \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6314f816-29e7-4d50-abd3-87f5fca2992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train =pd.read_csv('train.csv')    \n",
    "data_test_a = pd.read_csv('testA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5957b4-becc-48c4-a181-d7023883d1ce",
   "metadata": {},
   "source": [
    "## 3.1 特征预处理\n",
    "数据EDA部分我们已经对数据的大概和某些特征分布有了了解，数据预处理部分一般我们要处理一些EDA阶段分析出来的问题，这里介绍了数据缺失值的填充，时间格式特征的转化处理，某些对象类别特征的处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd36406-db50-4c27-9c16-245c9e491455",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_fea = list(data_train.select_dtypes(exclude=['object']).columns)    \n",
    "category_fea = list(filter(lambda x: x not in numerical_fea,list(data_train.columns)))    \n",
    "label = 'isDefault'    \n",
    "numerical_fea.remove(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ef751-e9de-4ff7-9cc5-593a02be309d",
   "metadata": {},
   "source": [
    "### 缺失值填充\n",
    "- 把所有缺失值替换为指定的值0\n",
    "\n",
    "data_train = data_train.fillna(0)\n",
    "\n",
    "- 向用缺失值上面的值替换缺失值\n",
    "\n",
    "data_train = data_train.fillna(axis=0,method='ffill')\n",
    "\n",
    "- 纵向用缺失值下面的值替换缺失值,且设置最多只填充两个连续的缺失值\n",
    "\n",
    "data_train = data_train.fillna(axis=0,method='bfill',limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0197b191-d50e-49cb-af8b-fcb66a847812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看缺失值情况    \n",
    "data_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2864737-26b8-4cd7-a072-b0ee78602cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#按照平均数填充数值型特征    \n",
    "data_train[numerical_fea] = data_train[numerical_fea].fillna(data_train[numerical_fea].median())    \n",
    "data_test_a[numerical_fea] = data_test_a[numerical_fea].fillna(data_train[numerical_fea].median())    \n",
    "#按照众数填充类别型特征    \n",
    "data_train[category_fea] = data_train[category_fea].fillna(data_train[category_fea].mode())    \n",
    "data_test_a[category_fea] = data_test_a[category_fea].fillna(data_train[category_fea].mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ba48b-8ef5-4408-a58f-f04780ed751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783fe1c-50b3-4287-93ad-f8fcb274a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['employmentLength'].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4428b33-c03e-45f9-bf89-dabb7a00f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def employmentLength_to_int(s):    \n",
    "    if pd.isnull(s):    \n",
    "        return s    \n",
    "    else:    \n",
    "        return np.int8(s.split()[0])    \n",
    "for data in [data_train, data_test_a]:    \n",
    "    data['employmentLength'].replace(to_replace='10+ years', value='10 years', inplace=True)    \n",
    "    data['employmentLength'].replace('< 1 year', '0 years', inplace=True)    \n",
    "    data['employmentLength'] = data['employmentLength'].apply(employmentLength_to_int)\n",
    "data['employmentLength'].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e8160-b698-4d9d-aae1-d1ed1ffd56c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#issueDate处理，转化成时间格式    \n",
    "for data in [data_train, data_test_a]:    \n",
    "    data['issueDate'] = pd.to_datetime(data['issueDate'],format='%Y-%m-%d')    \n",
    "    startdate = datetime.datetime.strptime('2007-06-01', '%Y-%m-%d')    \n",
    "    #构造时间特征    \n",
    "    data['issueDateDT'] = data['issueDate'].apply(lambda x: x-startdate).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d85fc-ddb9-447f-a85d-26af6c945799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对earliesCreditLine进行预处理\n",
    "data_train['earliesCreditLine'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73623740-de71-4fb8-9450-1d1d015efa32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf6239-d8ba-4d31-a2aa-11865f48cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "for data in [data_train, data_test_a]:\n",
    "    data['earliesCreditLine_mon'] = data['earliesCreditLine'].apply(lambda x: x.split('-')[0])\n",
    "    data['earliesCreditLine'] = data['earliesCreditLine'].apply(lambda s: int(s[-4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382621b9-0128-4c82-b6ce-a4c45f163efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 部分类别特征    \n",
    "cate_features = ['grade', 'subGrade', 'employmentTitle', 'homeOwnership', 'verificationStatus', 'purpose', 'postCode', 'regionCode',     \n",
    "                 'applicationType', 'initialListStatus', 'title', 'policyCode','earliesCreditLine_mon']    \n",
    "for f in cate_features:    \n",
    "    print(f, '类型数：', data[f].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78deefe3-9353-43df-876d-22ef4ef7f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#像等级这种类别特征，是有优先级的可以labelencode或者自映射\n",
    "for data in [data_train, data_test_a]:    \n",
    "    data['grade'] = data['grade'].map({'A':1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b16b7b7-a992-47a6-ba52-24030a2eeac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类型数在2之上，又不是高维稀疏的,且纯分类特征    \n",
    "for data in [data_train, data_test_a]:    \n",
    "    data = pd.get_dummies(data, columns=['subGrade', 'homeOwnership', 'verificationStatus', 'purpose', 'regionCode','earliesCreditLine_mon'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a583d2-556b-4a8d-8716-59a1c57dc88d",
   "metadata": {},
   "source": [
    "### 3.2 异常值处理\n",
    "- 当你发现异常值后，一定要先分清是什么原因导致的异常值，然后再考虑如何处理。首先，如果这一异常值并不代表一种规律性的，而是极其偶然的现象，或者说你并不想研究这种偶然的现象，这时可以将其删除。其次，如果异常值存在且代表了一种真实存在的现象，那就不能随便删除。在现有的欺诈场景中很多时候欺诈数据本身相对于正常数据勒说就是异常的，我们要把这些异常点纳入，重新拟合模型，研究其规律。能用监督的用监督模型，不能用的还可以考虑用异常检测的算法来做。\n",
    "\n",
    "- 注意test的数据不能删。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d09f97-7e44-4f7b-9001-ce50ce75cba6",
   "metadata": {},
   "source": [
    "### 3.2.1 检测异常的方法一：均方差\n",
    "在统计学中，如果一个数据分布近似正态，那么大约 68% 的数据值会在均值的一个标准差范围内，大约 95% 会在两个标准差范围内，大约 99.7% 会在三个标准差范围内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7564c2f-2567-44c8-8d17-02b205efcd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_by_3segama(data,fea):    \n",
    "    data_std = np.std(data[fea])    \n",
    "    data_mean = np.mean(data[fea])    \n",
    "    outliers_cut_off = data_std * 3    \n",
    "    lower_rule = data_mean - outliers_cut_off    \n",
    "    upper_rule = data_mean + outliers_cut_off    \n",
    "    data[fea+'_outliers'] = data[fea].apply(lambda x:str('异常值') if x > upper_rule or x < lower_rule else '正常值')    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc3ad62-0e0b-413f-96a2-204227aa8c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#得到特征的异常值后可以进一步分析变量异常值和目标变量的关系\n",
    "data_train = data_train.copy()    \n",
    "for fea in numerical_fea:    \n",
    "    data_train = find_outliers_by_3segama(data_train,fea)    \n",
    "    print(data_train[fea+'_outliers'].value_counts())    \n",
    "    print(data_train.groupby(fea+'_outliers')['isDefault'].sum())    \n",
    "    print('*'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d9135-2235-4811-b19b-8853d77acfc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 例如可以看到异常值在两个变量上的分布几乎复合整体的分布，如果异常值都属于为1的用户数据里面代表什么呢？\n",
    "#删除异常值    \n",
    "for fea in numerical_fea:    \n",
    "    data_train = data_train[data_train[fea+'_outliers']=='正常值']    \n",
    "    data_train = data_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f90a9c-ad22-4ef2-843e-c791272a1595",
   "metadata": {},
   "source": [
    "### 3.2.1检测异常的方法二：箱型图\n",
    "- 总结一句话：四分位数会将数据分为三个点和四个区间，IQR = Q3 -Q1，下触须=Q1 − 1.5x IQR，上触须=Q3 + 1.5x IQR；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f88a49d-e3c0-4f91-9ca7-d9b0942741ad",
   "metadata": {},
   "source": [
    "### 3.3 数据分桶\n",
    "- 特征分箱的目的：\n",
    "\n",
    "    - 从模型效果上来看，特征分箱主要是为了降低变量的复杂性，减少变量噪音对模型的影响，提高自变量和因变量的相关度。从而使模型更加稳定。\n",
    "- 数据分桶的对象：\n",
    "\n",
    "    - 将连续变量离散化\n",
    "    - 将多状态的离散变量合并成少状态\n",
    "- 分箱的原因：\n",
    "\n",
    "    - 数据的特征内的值跨度可能比较大，对有监督和无监督中如k-均值聚类它使用欧氏距离作为相似度函数来测量数据点之间的相似度。都会造成大吃小的影响，其中一种解决方法是对计数值进行区间量化即数据分桶也叫做数据分箱，然后使用量化后的结果。\n",
    "- 分箱的优点：\n",
    "\n",
    "- 处理缺失值：当数据源可能存在缺失值，此时可以把null单独作为一个分箱。\n",
    "- 处理异常值：当数据中存在离群点时，可以把其通过分箱离散化处理，从而提高变量的鲁棒性（抗干扰能力）。例如，age若出现200这种异常值，可分入“age > 60”这个分箱里，排除影响。\n",
    "- 业务解释性：我们习惯于线性判断变量的作用，当x越来越大，y就越来越大。但实际x与y之间经常存在着非线性关系，此时可经过WOE变换。\n",
    "- 特别要注意一下分箱的基本原则：\n",
    "\n",
    "- （1）最小分箱占比不低于5%\n",
    "- （2）箱内不能全部是好客户\n",
    "- （3）连续箱单调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ead26-5f46-4bff-b9fb-a623a691287a",
   "metadata": {},
   "source": [
    "1. 固定宽度分箱\n",
    "\n",
    "当数值横跨多个数量级时，最好按照 10 的幂（或任何常数的幂）来进行分组：0\\~9、10\\~99、100\\~999、1000\\~9999，等等。固定宽度分箱非常容易计算，但如果计数值中有比较大的缺口，就会产生很多没有任何数据的空箱子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b7af21-f8f9-4507-8e41-17a37a5c260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过除法映射到间隔均匀的分箱中，每个分箱的取值范围都是loanAmnt/1000    \n",
    "data['loanAmnt_bin1'] = np.floor_divide(data['loanAmnt'], 1000)\n",
    "#通过对数函数映射到指数宽度分箱    \n",
    "data['loanAmnt_bin2'] = np.floor(np.log10(data['loanAmnt']))\n",
    "#分位数分箱\n",
    "data['loanAmnt_bin3'] = pd.qcut(data['loanAmnt'], 10, labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86ba76-b2f2-4577-9e8a-52cca5f4eec7",
   "metadata": {},
   "source": [
    "### 3.4 特征交互\n",
    "\n",
    "交互特征的构造非常简单，使用起来却代价不菲。如果线性模型中包含有交互特征对，那它的训练时间和评分时间就会从 O(n) 增加到 O(n2)，其中 n 是单一特征的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b81e34-0fb7-47cb-b148-8f0f876b4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['grade', 'subGrade']:    \n",
    "    temp_dict = data_train.groupby([col])['isDefault'].agg(['mean']).reset_index().rename(columns={'mean': col + '_target_mean'})    \n",
    "    temp_dict.index = temp_dict[col].values    \n",
    "    temp_dict = temp_dict[col + '_target_mean'].to_dict()    \n",
    "\n",
    "    data_train[col + '_target_mean'] = data_train[col].map(temp_dict)    \n",
    "    data_test_a[col + '_target_mean'] = data_test_a[col].map(temp_dict)\n",
    "    \n",
    "# 其他衍生变量 mean 和 std    \n",
    "for df in [data_train, data_test_a]:    \n",
    "    for item in ['n0','n1','n2','n3','n4','n5','n6','n7','n8','n9','n10','n11','n12','n13','n14']:    \n",
    "        df['grade_to_mean_' + item] = df['grade'] / df.groupby([item])['grade'].transform('mean')    \n",
    "        df['grade_to_std_' + item] = df['grade'] / df.groupby([item])['grade'].transform('std')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab7cbb-8c17-43a0-8fe2-fc437e2ecfda",
   "metadata": {},
   "source": [
    "## 3.5 特征编码\n",
    "### 3.5.1labelEncode 直接放入树模型中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea4162-7e31-4720-bdd2-26c704ad4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label-encode:subGrade,postCode,title    \n",
    "#高维类别特征需要进行转换    \n",
    "for col in tqdm(['employmentTitle', 'postCode', 'title','subGrade']):    \n",
    "    le = LabelEncoder()    \n",
    "    le.fit(list(data_train[col].astype(str).values) + list(data_test_a[col].astype(str).values))    \n",
    "    data_train[col] = le.transform(list(data_train[col].astype(str).values))    \n",
    "    data_test_a[col] = le.transform(list(data_test_a[col].astype(str).values))    \n",
    "print('Label Encoding 完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708db168-ba50-4a02-814c-4ed68b88eb95",
   "metadata": {},
   "source": [
    "### 3.5.2逻辑回归等模型要单独增加的特征工程\n",
    "- 对特征做归一化，去除相关性高的特征\n",
    "- 归一化目的是让训练过程更好更快的收敛，避免特征大吃小的问题\n",
    "- 去除相关性是增加模型的可解释性，加快预测过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a72b57-5a22-4113-8891-c738a6962cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 举例归一化过程    \n",
    "#伪代码    \n",
    "for fea in [要归一化的特征列表]：    \n",
    "    data[fea] = ((data[fea] - np.min(data[fea])) / (np.max(data[fea]) - np.min(data[fea])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76f360-3df2-4802-8c7d-ce252c6c8bc9",
   "metadata": {},
   "source": [
    "## 3.6 特征选择\n",
    "- 特征选择技术可以精简掉无用的特征，以降低最终模型的复杂性，它的最终目的是得到一个简约模型，在不降低预测准确率或对预测准确率影响不大的情况下提高计算速度。特征选择不是为了减少训练时间（实际上，一些技术会增加总体训练时间），而是为了减少模型评分时间。\n",
    "特征选择的方法：\n",
    "\n",
    "- 1 Filter\n",
    "    - 方差选择法\n",
    "    - 相关系数法（pearson 相关系数）\n",
    "    - 卡方检验\n",
    "    - 互信息法\n",
    "- 2 Wrapper （RFE）\n",
    "    - 递归特征消除法\n",
    "- 3 Embedded\n",
    "    - 基于惩罚项的特征选择法\n",
    "    - 基于树模型的特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1380f141-c445-428a-92fd-970390b8cff5",
   "metadata": {},
   "source": [
    "### 3.6.1Filter（方差选择法、相关系数法）\n",
    "- 基于特征间的关系进行筛选\n",
    "\n",
    "方差选择法\n",
    "\n",
    "- 方差选择法中，先要计算各个特征的方差，然后根据设定的阈值，选择方差大于阈值的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8df22-5a06-47f5-b1b1-7a720dae558a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold    \n",
    "#其中参数threshold为方差的阈值    \n",
    "VarianceThreshold(threshold=3).fit_transform(train,target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70deae23-e7e6-46cd-8cca-4b4ac23b4ff2",
   "metadata": {},
   "source": [
    "- 相关系数法\n",
    "\n",
    "- Pearson 相关系数\n",
    "    皮尔森相关系数是一种最简单的，可以帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性。\n",
    "    结果的取值区间为 [-1，1] ， -1 表示完全的负相关， +1表示完全的正相关，0 表示没有线性相关。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde0595c-bf62-4ca3-b4ba-9a66a476db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest    \n",
    "from scipy.stats import pearsonr    \n",
    "#选择K个最好的特征，返回选择特征后的数据    \n",
    "#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，    \n",
    "#输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数    \n",
    "#参数k为选择的特征个数    \n",
    "\n",
    "SelectKBest(k=5).fit_transform(train,target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a33e2-6c14-4dee-a9a1-d819b819fcc4",
   "metadata": {},
   "source": [
    "卡方检验\n",
    "\n",
    "- 经典的卡方检验是用于检验自变量对因变量的相关性。 假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距。 其统计量如下： χ2=∑(A−T)2T，其中A为实际值，T为理论值\n",
    "\n",
    "(注：卡方只能运用在正定矩阵上，否则会报错Input X must be non-negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639ef31e-30f3-4e7b-bd6e-63287bf061e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rom sklearn.feature_selection import SelectKBest    \n",
    "from sklearn.feature_selection import chi2    \n",
    "#参数k为选择的特征个数    \n",
    "\n",
    "SelectKBest(chi2, k=5).fit_transform(train,target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef1427-1d1f-4146-a1d3-dd683a61f1cc",
   "metadata": {},
   "source": [
    "互信息法\n",
    "\n",
    "- 经典的互信息也是评价自变量对因变量的相关性的。 在feature_selection库的SelectKBest类结合最大信息系数法可以用于选择特征，相关代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb539f-7ee4-4ac2-a5ba-b800869dd74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest    \n",
    "from minepy import MINE    \n",
    "#由于MINE的设计不是函数式的，定义mic方法将其为函数式的，    \n",
    "#返回一个二元组，二元组的第2项设置成固定的P值0.5    \n",
    "def mic(x, y):    \n",
    "    m = MINE()    \n",
    "    m.compute_score(x, y)    \n",
    "    return (m.mic(), 0.5)    \n",
    "#参数k为选择的特征个数    \n",
    "SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(train,target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a17e5-b321-4d20-9119-43a8d2555972",
   "metadata": {},
   "source": [
    "### 3.6.2Wrapper （递归特征法）\n",
    "- 递归特征消除法 递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。 在feature_selection库的RFE类可以用于选择特征，相关代码如下（以逻辑回归为例）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3be0212-cc89-4a85-b9de-efd2a1077cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE    \n",
    "from sklearn.linear_model import LogisticRegression    \n",
    "#递归特征消除法，返回特征选择后的数据    \n",
    "#参数estimator为基模型    \n",
    "#参数n_features_to_select为选择的特征个数    \n",
    "\n",
    "RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(train,target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f60d1f1-1a40-4f28-8e94-b8c394f02bd9",
   "metadata": {},
   "source": [
    "### 3.6.3Embedded（ 惩罚项的特征选择法、树模型的特征选择）\n",
    "- 基于惩罚项的特征选择法 使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。 在feature_selection库的SelectFromModel类结合逻辑回归模型可以用于选择特征，相关代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae9acae-6931-4c2a-b2f8-434ff2f36925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel    \n",
    "from sklearn.linear_model import LogisticRegression    \n",
    "#带L1惩罚项的逻辑回归作为基模型的特征选择    \n",
    "\n",
    "SelectFromModel(LogisticRegression(penalty=\"l1\", C=0.1)).fit_transform(train,target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87885c9a-e1fd-41c5-b2d2-882945f7a07b",
   "metadata": {},
   "source": [
    "- 基于树模型的特征选择 树模型中GBDT也可用来作为基模型进行特征选择。 在feature_selection库的SelectFromModel类结合GBDT模型可以用于选择特征，相关代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561fb53f-fcf5-4e84-991e-530274e8f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel    \n",
    "from sklearn.ensemble import GradientBoostingClassifier    \n",
    "#GBDT作为基模型的特征选择    \n",
    "SelectFromModel(GradientBoostingClassifier()).fit_transform(train,target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c04fe06-58bf-4e91-b7e1-777b2f39a761",
   "metadata": {},
   "source": [
    "本数据集中我们删除非入模特征后，并对缺失值填充，然后用计算协方差的方式看一下特征间相关性，然后进行模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd08d8-4681-4615-93a8-b5c0d10ded38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除不需要的数据    \n",
    "for data in [data_train, data_test_a]:    \n",
    "    data.drop(['issueDate','id'], axis=1,inplace=True)\n",
    "# \"纵向用缺失值上面的值替换缺失值\"    \n",
    "data_train = data_train.fillna(axis=0,method='ffill')\n",
    "x_train = data_train.drop(['isDefault','id'], axis=1)    \n",
    "#计算协方差    \n",
    "data_corr = x_train.corrwith(data_train.isDefault) #计算相关性    \n",
    "result = pd.DataFrame(columns=['features', 'corr'])    \n",
    "result['features'] = data_corr.index    \n",
    "result['corr'] = data_corr.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450ac23-e3a7-4257-ace5-382f3d413a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当然也可以直接看图    \n",
    "data_numeric = data_train[numerical_fea]    \n",
    "correlation = data_numeric.corr()    \n",
    "\n",
    "f , ax = plt.subplots(figsize = (7, 7))    \n",
    "plt.title('Correlation of Numeric Features with Price',y=1,size=16)    \n",
    "sns.heatmap(correlation,square = True,  vmax=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa89904d-1a6c-42b2-8629-4edcd4a34ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [f for f in data_train.columns if f not in ['id','issueDate','isDefault','earliesCreditLine_mon','subGrade'] and '_outliers' not in f]    \n",
    "x_train = data_train[features]    \n",
    "x_test = data_test_a[features]    \n",
    "y_train = data_train['isDefault']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edcdfe4-0fba-4cb6-b581-4b673c42a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['subGrade'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee30a2cc-804b-46fc-986c-7235d9e785b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_model(clf, train_x, train_y, test_x, clf_name):    \n",
    "    folds = 5    \n",
    "    seed = 2020    \n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)    \n",
    "\n",
    "    train = np.zeros(train_x.shape[0])    \n",
    "    test = np.zeros(test_x.shape[0])    \n",
    "\n",
    "    cv_scores = []    \n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):    \n",
    "        print('************************************ {} ************************************'.format(str(i+1)))    \n",
    "        trn_x, trn_y, val_x, val_y = train_x.iloc[train_index], train_y[train_index], train_x.iloc[valid_index], train_y[valid_index]    \n",
    "\n",
    "        if clf_name == \"lgb\":    \n",
    "            train_matrix = clf.Dataset(trn_x, label=trn_y)    \n",
    "            valid_matrix = clf.Dataset(val_x, label=val_y)    \n",
    "\n",
    "            params = {    \n",
    "                'boosting_type': 'gbdt',    \n",
    "                'objective': 'binary',    \n",
    "                'metric': 'auc',    \n",
    "                'min_child_weight': 5,    \n",
    "                'num_leaves': 2 ** 5,    \n",
    "                'lambda_l2': 10,    \n",
    "                'feature_fraction': 0.8,    \n",
    "                'bagging_fraction': 0.8,    \n",
    "                'bagging_freq': 4,    \n",
    "                'learning_rate': 0.1,    \n",
    "                'seed': 2020,    \n",
    "                'nthread': 24,    \n",
    "                'n_jobs':24,    \n",
    "                'silent': True,    \n",
    "                'verbose': -1,    \n",
    "            }    \n",
    "\n",
    "            model = clf.train(params, train_matrix, 50000, valid_sets=[train_matrix, valid_matrix], verbose_eval=200,early_stopping_rounds=200)    \n",
    "            val_pred = model.predict(val_x, num_iteration=model.best_iteration)    \n",
    "            test_pred = model.predict(test_x, num_iteration=model.best_iteration)    \n",
    "\n",
    "            # print(list(sorted(zip(features, model.feature_importance(\"gain\")), key=lambda x: x[1], reverse=True))[:20])    \n",
    "\n",
    "        if clf_name == \"xgb\":    \n",
    "            train_matrix = clf.DMatrix(trn_x , label=trn_y)    \n",
    "            valid_matrix = clf.DMatrix(val_x , label=val_y)    \n",
    "\n",
    "            params = {'booster': 'gbtree',    \n",
    "                      'objective': 'binary:logistic',    \n",
    "                      'eval_metric': 'auc',    \n",
    "                      'gamma': 1,    \n",
    "                      'min_child_weight': 1.5,    \n",
    "                      'max_depth': 5,    \n",
    "                      'lambda': 10,    \n",
    "                      'subsample': 0.7,    \n",
    "                      'colsample_bytree': 0.7,    \n",
    "                      'colsample_bylevel': 0.7,    \n",
    "                      'eta': 0.04,    \n",
    "                      'tree_method': 'exact',    \n",
    "                      'seed': 2020,    \n",
    "                      'nthread': 36,    \n",
    "                      \"silent\": True,    \n",
    "                      }    \n",
    "\n",
    "            watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]    \n",
    "\n",
    "            model = clf.train(params, train_matrix, num_boost_round=50000, evals=watchlist, verbose_eval=200, early_stopping_rounds=200)    \n",
    "            val_pred  = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)    \n",
    "            test_pred = model.predict(test_x , ntree_limit=model.best_ntree_limit)    \n",
    "\n",
    "        if clf_name == \"cat\":    \n",
    "            params = {'learning_rate': 0.05, 'depth': 5, 'l2_leaf_reg': 10, 'bootstrap_type': 'Bernoulli',    \n",
    "                      'od_type': 'Iter', 'od_wait': 50, 'random_seed': 11, 'allow_writing_files': False}    \n",
    "\n",
    "            model = clf(iterations=20000, **params)    \n",
    "            model.fit(trn_x, trn_y, eval_set=(val_x, val_y),    \n",
    "                      cat_features=[], use_best_model=True, verbose=500)    \n",
    "\n",
    "            val_pred  = model.predict(val_x)    \n",
    "            test_pred = model.predict(test_x)    \n",
    "\n",
    "        train[valid_index] = val_pred    \n",
    "        test = test_pred / kf.n_splits    \n",
    "        cv_scores.append(roc_auc_score(val_y, val_pred))    \n",
    "\n",
    "        print(cv_scores)    \n",
    "\n",
    "    print(\"%s_scotrainre_list:\" % clf_name, cv_scores)    \n",
    "    print(\"%s_score_mean:\" % clf_name, np.mean(cv_scores))    \n",
    "    print(\"%s_score_std:\" % clf_name, np.std(cv_scores))    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee4659e-8d43-4f68-bca3-4a3cac94b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_model(x_train, y_train, x_test):    \n",
    "    lgb_train, lgb_test = cv_model(lgb, x_train, y_train, x_test, \"lgb\")    \n",
    "    return lgb_train, lgb_test    \n",
    "\n",
    "def xgb_model(x_train, y_train, x_test):    \n",
    "    xgb_train, xgb_test = cv_model(xgb, x_train, y_train, x_test, \"xgb\")    \n",
    "    return xgb_train, xgb_test    \n",
    "\n",
    "def cat_model(x_train, y_train, x_test):    \n",
    "    cat_train, cat_test = cv_model(CatBoostRegressor, x_train, y_train, x_test, \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe1456-08b1-41bb-95d7-f3404722a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train, lgb_test = lgb_model(x_train, y_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f007f515-4552-4cc1-8194-3caea4a85c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "testA_result.to_pickle('testA_result.pickle')\n",
    "roc_auc_score(lgb_test['isDefault'].values, lgb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc01979-39f1-4c22-bfea-221c025c0747",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_test = pd.DataFrame([[data_test_a['id'][i], lgb_test[i]] for i in range(len(lgb_test))], columns=['id', 'isDefault'])\n",
    "lgb_test.to_csv('lgb_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
