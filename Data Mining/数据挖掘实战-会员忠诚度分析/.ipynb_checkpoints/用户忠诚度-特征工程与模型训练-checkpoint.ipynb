{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、特征工程\n",
    "\n",
    "首先需要对得到的数据进一步进行特征工程处理。一般来说，对于已经清洗完的数据，特征工程部分核心需要考虑的问题就是特征创建（衍生）与特征筛选，也就是先尽可能创建/增加可能对模型结果有正面影响的特征，然后再对这些进行挑选，以保证模型运行稳定性及运行效率。当然，无论是特征衍生还是特征筛选，其实都有非常多的方法。此处为了保证公开课提供的思路和方法具有通用性，此处列举两种特征衍生的方法，即**创建通用组合特征与业务统计特征**；并在特征创建完毕后，介绍一种基础而通用的特征筛选的方法：基于**皮尔逊相关系数的Filter方法进行特征筛选**。这些方法都是非常通用且有效的方法，不仅能够帮助本次建模取得较好的成果，并且也能广泛适用到其他各场景中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.通用组合特征创建\n",
    "#### 1.1 通用组合特征的创建方法\n",
    "\n",
    "所谓通用组合特征，指的是通过统计不同离散特征在不同取值水平下、不同连续特征取值之和创建的特征，并根据card_id进行分组求和。具体创建过程我们可以如下简例来进行理解："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/23/KUTtb32o9eS1MJW.png\" alt=\"image-20211023153800138\" style=\"zoom:67%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   card_id  A  B  C  D\n",
       "0        1  1  2  4  7\n",
       "1        2  2  1  5  5\n",
       "2        1  1  2  1  4\n",
       "3        3  2  2  5  8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 例子\n",
    "d1 = {'card_id':[1, 2, 1, 3], \n",
    "      'A':[1, 2, 1, 2], \n",
    "      'B':[2, 1, 2, 2], \n",
    "      'C':[4, 5, 1, 5], \n",
    "      'D':[7, 5, 4, 8],}\n",
    "\n",
    "t1 = pd.DataFrame(d1)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标注特征类别\n",
    "numeric_cols = ['C', 'D']\n",
    "category_cols = ['A', 'B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {}, 2: {}, 3: {}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个以id为key、空字典为value的字典\n",
    "features = {}\n",
    "card_all = t1['card_id'].values.tolist()\n",
    "for card in card_all:\n",
    "    features[card] = {}\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取card_id在列中的位置\n",
    "column = t1.columns.tolist()\n",
    "idx_card = column.index('card_id')\n",
    "idx_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 离散型特征的位置\n",
    "category_cols_index = [column.index(col) for col in category_cols]\n",
    "numeric_cols_index = [column.index(col) for col in numeric_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'A&1&C': 5, 'A&1&D': 11, 'B&2&C': 5, 'B&2&D': 11},\n",
       " 2: {'A&2&C': 5, 'A&2&D': 5, 'B&1&C': 5, 'B&1&D': 5},\n",
       " 3: {'A&2&C': 5, 'A&2&D': 8, 'B&2&C': 5, 'B&2&D': 8}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(t1.shape[0]):\n",
    "    line_value = t1.loc[_].values\n",
    "    card_id = line_value[idx_card]\n",
    "    for category_col in category_cols_index:\n",
    "        for numeric_col in numeric_cols_index:\n",
    "            feature = '&'.join([column[category_col],str(line_value[category_col]),column[numeric_col]])\n",
    "            features[card_id][feature] = features[card_id].get(feature,0)+line_value[numeric_col]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>A&amp;1&amp;C</th>\n",
       "      <th>A&amp;1&amp;D</th>\n",
       "      <th>B&amp;2&amp;C</th>\n",
       "      <th>B&amp;2&amp;D</th>\n",
       "      <th>A&amp;2&amp;C</th>\n",
       "      <th>A&amp;2&amp;D</th>\n",
       "      <th>B&amp;1&amp;C</th>\n",
       "      <th>B&amp;1&amp;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   card_id  A&1&C  A&1&D  B&2&C  B&2&D  A&2&C  A&2&D  B&1&C  B&1&D\n",
       "0        1    5.0   11.0    5.0   11.0    NaN    NaN    NaN    NaN\n",
       "1        2    NaN    NaN    NaN    NaN    5.0    5.0    5.0    5.0\n",
       "2        3    NaN    NaN    5.0    8.0    5.0    8.0    NaN    NaN"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转化成df\n",
    "df = pd.DataFrame(features).T.reset_index()\n",
    "df = df.rename(columns={'index':'card_id'})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 基于transaction数据集创建通用组合特征\n",
    "    数据量过大，此处不跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:\\数据挖掘实战\\Kaggle实例-3\\案例一【Elo用户忠诚度预测】\\Part 1\\数据\\数据清洗后数据\\\\'\n",
    "train = pd.read_csv(path+'train_pre.csv')\n",
    "test =  pd.read_csv(path+'test_pre.csv')\n",
    "transaction = pd.read_csv(path+'transaction_pre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标注离散字段or连续型字段\n",
    "numeric_cols = ['purchase_amount', 'installments']\n",
    "\n",
    "category_cols = ['authorized_flag', 'city_id', 'category_1',\n",
    "       'category_3', 'merchant_category_id','month_lag','most_recent_sales_range',\n",
    "                 'most_recent_purchases_range', 'category_4',\n",
    "                 'purchase_month', 'purchase_hour_section', 'purchase_day']\n",
    "\n",
    "id_cols = ['card_id', 'merchant_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建字典用于保存数据\n",
    "features = {}\n",
    "card_all = train['card_id'].append(test['card_id']).values.tolist()\n",
    "for card in card_all:\n",
    "    features[card] = {}\n",
    "     \n",
    "# 标记不同类型字段的索引\n",
    "columns = transaction.columns.tolist()\n",
    "idx = columns.index('card_id')\n",
    "category_cols_index = [columns.index(col) for col in category_cols]\n",
    "numeric_cols_index = [columns.index(col) for col in numeric_cols]\n",
    "\n",
    "# 记录运行时间\n",
    "s = time.time()\n",
    "num = 0\n",
    "\n",
    "# 执行循环，并在此过程中记录时间\n",
    "for i in range(transaction.shape[0]):\n",
    "    va = transaction.loc[i].values\n",
    "    card = va[idx]\n",
    "    for cate_ind in category_cols_index:\n",
    "        for num_ind in numeric_cols_index:\n",
    "            col_name = '&'.join([columns[cate_ind], va[cate_ind], columns[num_ind]])\n",
    "            features[card][col_name] = features[card].get(col_name, 0) + va[num_ind]\n",
    "    num += 1\n",
    "    if num%1000000==0:\n",
    "        print(time.time()-s, \"s\")\n",
    "del transaction\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字典转dataframe\n",
    "df = pd.DataFrame(features).T.reset_index()\n",
    "del features\n",
    "cols = df.columns.tolist()\n",
    "df.columns = ['card_id'] + cols[1:]\n",
    "\n",
    "# 生成训练集与测试集\n",
    "train = pd.merge(train, df, how='left', on='card_id')\n",
    "test =  pd.merge(test, df, how='left', on='card_id')\n",
    "del df\n",
    "train.to_csv(path+\"train_dict.csv\", index=False)\n",
    "test.to_csv(path+\"test_dict.csv\", index=False)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.业务统计特征创建\n",
    "\n",
    "除了通用组合特征外，我们还可以考虑从另一个角度进行特征提取，那就是先根据card_id来进行分组，然后统计不同字段再各组内的相关统计量，再将其作为特征，带入进行建模。其基本构造特征思路如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/23/NupDc9JnBbHRPgU.png\" alt=\"image-20211023162730619\" style=\"zoom:80%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建空字典\n",
    "aggs = {}\n",
    "\n",
    "# 连续/离散字段统计量提取范围\n",
    "for col in numeric_cols:\n",
    "    aggs[col] = ['nunique', 'mean', 'min', 'max','var','skew', 'sum']\n",
    "for col in categorical_cols:\n",
    "    aggs[col] = ['nunique']    \n",
    "aggs['card_id'] = ['size', 'count']\n",
    "cols = ['card_id']\n",
    "\n",
    "# 借助groupby实现统计量计算\n",
    "for key in aggs.keys():\n",
    "    cols.extend([key+'_'+stat for stat in aggs[key]])\n",
    "\n",
    "df = transaction[transaction['month_lag']<0].groupby('card_id').agg(aggs).reset_index()\n",
    "df.columns = cols[:1] + [co+'_hist' for co in cols[1:]]\n",
    "\n",
    "df2 = transaction[transaction['month_lag']>=0].groupby('card_id').agg(aggs).reset_index()\n",
    "df2.columns = cols[:1] + [co+'_new' for co in cols[1:]]\n",
    "df = pd.merge(df, df2, how='left',on='card_id')\n",
    "\n",
    "df2 = transaction.groupby('card_id').agg(aggs).reset_index()\n",
    "df2.columns = cols\n",
    "df = pd.merge(df, df2, how='left',on='card_id')\n",
    "del transaction\n",
    "gc.collect()\n",
    "\n",
    "# 生成训练集与测试集\n",
    "train = pd.merge(train, df, how='left', on='card_id')\n",
    "test =  pd.merge(test, df, how='left', on='card_id')\n",
    "del df\n",
    "train.to_csv(path+\"train_groupby.csv\", index=False)\n",
    "test.to_csv(path+\"test_groupby.csv\", index=False)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.数据合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = pd.read_csv(path+\"train_dict.csv\")\n",
    "test_dict = pd.read_csv(path+\"test_dict.csv\")\n",
    "train_groupby = pd.read_csv(path+\"train_groupby.csv\")\n",
    "test_groupby = pd.read_csv(path+\"test_groupby.csv\")\n",
    "# 剔除重复列\n",
    "for co in train_dict.columns:\n",
    "    if co in train_groupby.columns and co!='card_id':\n",
    "        del train_groupby[co]\n",
    "for co in test_dict.columns:\n",
    "    if co in test_groupby.columns and co!='card_id':\n",
    "        del test_groupby[co]\n",
    "# 拼接特征\n",
    "train = pd.merge(train_dict, train_groupby, how='left', on='card_id').fillna(0)\n",
    "test = pd.merge(test_dict, test_groupby, how='left', on='card_id').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"preprocess/train.csv\", index=False)\n",
    "test.to_csv(\"preprocess/test.csv\", index=False)\n",
    "\n",
    "del train_dict, test_dict, train_groupby, test_groupby\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、随机森林模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path= 'D:\\数据挖掘实战\\Kaggle实例-3\\案例一【Elo用户忠诚度预测】\\Part 1\\数据\\建模数据\\\\'\n",
    "train = pd.read_csv(path+\"train.csv\")\n",
    "test = pd.read_csv(path+\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;由于此前创建了数千条特征，若带入全部特征进行建模，势必极大程度延长模型建模时间，并且带入太多无关特征对模型结果提升有限，因此此处我们借助皮尔逊相关系数，挑选和标签最相关的300个特征进行建模。当然此处300也可以自行调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 提取特征名称\n",
    "features = train.columns.tolist()\n",
    "features.remove('card_id')\n",
    "features.remove('target')\n",
    "featureSelect = features[:]\n",
    "\n",
    "# 计算与标签的相关系数\n",
    "corr = []\n",
    "for feature in featureSelect:\n",
    "    corr.append(abs(train[[feature,'target']].fillna(0).corr().values[0][1]))\n",
    "\n",
    "# 取top300的特征\n",
    "feature_corr = pd.Series(corr, index=featureSelect).sort_values(ascending=False)\n",
    "feature_se = ['card_id']+feature_corr[:300].index.tolist()\n",
    "\n",
    "# 输出结果\n",
    "train = train[feature_se+['target']]\n",
    "test = test[feature_se]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，此处可以通过皮尔逊相关系数进行特征提取的主要原因也是在于我们在特征创建的过程中，将所有特征都默认为连续性变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 借助网格搜索进行参数调优，sklearn中基础调参工具—网格搜索（Gridsearch）进行参数搜索与调优\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后根据网格搜索的要求，我们需要根据随机森林的参数情况，有针对性的创造一个参数空间，随机森林基本参数基本情况如下：\n",
    "\n",
    "|Name|Description|      \n",
    "|:--:|:--:| \n",
    "|criterion|规则评估指标或损失函数，默认基尼系数，可选信息熵| \n",
    "|splitter|树模型生长方式，默认以损失函数取值减少最快方式生长，可选随机根据某条件进行划分|\n",
    "|max_depth|树的最大生长深度，类似max_iter，即总共迭代几次| \n",
    "|min_samples_split|内部节点再划分所需最小样本数| \n",
    "|min_samples_leaf|叶节点包含最少样本数| \n",
    "|min_weight_fraction_leaf|叶节点所需最小权重和| \n",
    "|max_features|在进行切分时候最多带入多少个特征进行划分规则挑选|\n",
    "|random_state|随机数种子| \n",
    "|max_leaf_nodes|叶节点最大个数| \n",
    "|min_impurity_decrease|数据集再划分至少需要降低的损失值| \n",
    "|min_impurity_split|数据集再划分所需最低不纯度，将在0.25版本中移除| \n",
    "|class_weight|各类样本权重| \n",
    "\n",
    "其中我们挑选\"n_estimators\"、\"min_samples_leaf\"、\"min_samples_split\"、\"max_depth\"和\"max_features\"进行参数搜索："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后是关于网格搜索工具的选择。随着sklearn不断完善，有越来越多的网格搜索工具可供选择，但整体来看其实就是在效率和精度之间做权衡，有些网格搜索工具由于是全域枚举（如GridSearchCV），所以执行效率较慢、但结果精度有保障，而如果愿意牺牲精度换执行效率，则也有很多工具可以选择，如RandomizedSearchCV。当然，在最新的sklearn版本中，还出现了一种更高效的搜索策略——HalvingGridSearchCV，该方法先两两比对、然后逐层筛选的方法来进行参数筛选，并且同时支持HalvingGridSearchCV和HalvingRandomSearchCV。注意，这是sklearn最新版、也就是0.24版才支持的功能，该功能的出现也是0.24版最大的改动之一，而该功能的加入，也将进一步减少网格搜索所需计算资源、加快网格搜索的速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train.columns.tolist()\n",
    "features.remove('card_id')\n",
    "features.remove('target')\n",
    "\n",
    "parameter_space = {\n",
    "    \"n_estimators\": [79, 80, 81], \n",
    "    \"min_samples_leaf\": [29, 30, 31],\n",
    "    \"min_samples_split\": [2, 3],\n",
    "    \"max_depth\": [9, 10],\n",
    "    \"max_features\": [\"auto\", 80]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(\n",
    "    criterion='mse',\n",
    "    n_jobs=15,\n",
    "    random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始网格搜索\n",
    "grid = GridSearchCV(clf, parameter_space, cv=2, scoring='neg_mean_squared_error')\n",
    "grid.fit(train[features].values, train['target'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取最优参数\n",
    "grid.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看最优参数组成评估器\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集评分\n",
    "np.sqrt(-grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行预测\n",
    "test['target'] = grid.best_estimator_.predict(test[features])\n",
    "test[['card_id','target']].to_csv('submission_randomforest.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、后续优化策略\n",
    "    - 文本特征挖掘\n",
    "    在特征处理的过程中，可以尝试使用NLP领域的TF-IDF进行词频统计，增加离散变量特征；\n",
    "    - 更多衍生特征\n",
    "    除了对离散变量进行词频统计外，我们还可以考虑构建更多特征，如全局card_id特征、最近两个月  card_id特征、二阶特征和补充特征等，来更深程度挖掘数据集信息；\n",
    "    - 更多集成算法\n",
    "    除了随机森林外，还有许多功能非常强大的集成模型，包括LightGBM、XGBoost等，都是可以尝试使用的算法；\n",
    "    - 模型融合方法\n",
    "    既然使用了多集成模型来进行建模，那么模型融合也势在必行。模型融合能够很好的综合各集成模型的输出结果，来做出最后更加综合的判断。当然模型融合可以考虑简单加权融合或者stacking融合方法；\n",
    "    - 更加细致的数据处理\n",
    "    除了技术手段外，我们可也可以围绕此前得出的业务分析结论，对数据集进行更加细致的处理，如此前标签中出现的异常值的处理、13家商户没有过去一段时间营销信息等，通过更加细致的处理，能够让模型达到更好的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
